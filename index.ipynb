{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Regularization-and-Optimization-of-Neural-Networks---Lab\" data-toc-modified-id=\"Regularization-and-Optimization-of-Neural-Networks---Lab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Regularization and Optimization of Neural Networks - Lab</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Import-the-libraries\" data-toc-modified-id=\"Import-the-libraries-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Import the libraries</a></span></li><li><span><a href=\"#Load-the-Data\" data-toc-modified-id=\"Load-the-Data-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Load the Data</a></span></li><li><span><a href=\"#Preprocessing-Overview\" data-toc-modified-id=\"Preprocessing-Overview-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Preprocessing Overview</a></span></li><li><span><a href=\"#Preprocessing:-Generate-a-Random-Sample\" data-toc-modified-id=\"Preprocessing:-Generate-a-Random-Sample-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Preprocessing: Generate a Random Sample</a></span></li><li><span><a href=\"#Preprocessing:-One-hot-Encoding-of-the-Complaints\" data-toc-modified-id=\"Preprocessing:-One-hot-Encoding-of-the-Complaints-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Preprocessing: One-hot Encoding of the Complaints</a></span></li><li><span><a href=\"#Preprocessing:-Encoding-the-Products\" data-toc-modified-id=\"Preprocessing:-Encoding-the-Products-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Preprocessing: Encoding the Products</a></span></li><li><span><a href=\"#Train-test-Split\" data-toc-modified-id=\"Train-test-Split-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Train-test Split</a></span></li><li><span><a href=\"#Running-the-model-using-a-validation-set.\" data-toc-modified-id=\"Running-the-model-using-a-validation-set.-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Running the model using a validation set.</a></span></li><li><span><a href=\"#Creating-the-Validation-Set\" data-toc-modified-id=\"Creating-the-Validation-Set-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Creating the Validation Set</a></span></li><li><span><a href=\"#Creating-the-Model\" data-toc-modified-id=\"Creating-the-Model-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>Creating the Model</a></span></li><li><span><a href=\"#Compiling-the-Model\" data-toc-modified-id=\"Compiling-the-Model-1.13\"><span class=\"toc-item-num\">1.13&nbsp;&nbsp;</span>Compiling the Model</a></span></li><li><span><a href=\"#Part-2:-Code-Along\" data-toc-modified-id=\"Part-2:-Code-Along-1.14\"><span class=\"toc-item-num\">1.14&nbsp;&nbsp;</span>Part 2: Code Along</a></span></li><li><span><a href=\"#Training-the-Model\" data-toc-modified-id=\"Training-the-Model-1.15\"><span class=\"toc-item-num\">1.15&nbsp;&nbsp;</span>Training the Model</a></span></li><li><span><a href=\"#Retrieving-Performance-Results:-the-history-dictionary\" data-toc-modified-id=\"Retrieving-Performance-Results:-the-history-dictionary-1.16\"><span class=\"toc-item-num\">1.16&nbsp;&nbsp;</span>Retrieving Performance Results: the <code>history</code> dictionary</a></span></li><li><span><a href=\"#Plotting-the-Results\" data-toc-modified-id=\"Plotting-the-Results-1.17\"><span class=\"toc-item-num\">1.17&nbsp;&nbsp;</span>Plotting the Results</a></span></li><li><span><a href=\"#Early-Stopping\" data-toc-modified-id=\"Early-Stopping-1.18\"><span class=\"toc-item-num\">1.18&nbsp;&nbsp;</span>Early Stopping</a></span></li><li><span><a href=\"#L2-Regularization\" data-toc-modified-id=\"L2-Regularization-1.19\"><span class=\"toc-item-num\">1.19&nbsp;&nbsp;</span>L2 Regularization</a></span></li><li><span><a href=\"#L1-Regularization\" data-toc-modified-id=\"L1-Regularization-1.20\"><span class=\"toc-item-num\">1.20&nbsp;&nbsp;</span>L1 Regularization</a></span></li><li><span><a href=\"#Dropout-Regularization\" data-toc-modified-id=\"Dropout-Regularization-1.21\"><span class=\"toc-item-num\">1.21&nbsp;&nbsp;</span>Dropout Regularization</a></span></li><li><span><a href=\"#Bigger-Data?\" data-toc-modified-id=\"Bigger-Data?-1.22\"><span class=\"toc-item-num\">1.22&nbsp;&nbsp;</span>Bigger Data?</a></span></li><li><span><a href=\"#Additional-Resources\" data-toc-modified-id=\"Additional-Resources-1.23\"><span class=\"toc-item-num\">1.23&nbsp;&nbsp;</span>Additional Resources</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-1.24\"><span class=\"toc-item-num\">1.24&nbsp;&nbsp;</span>Summary</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df['Product']\n",
    "complaints = df['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "le = LabelEncoder()\n",
    "le.fit(product)\n",
    "prod_cat = le.transform(product)\n",
    "\n",
    "onehot_prod = to_categorical(prod_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, onehot_prod, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.9549 - acc: 0.1588 - val_loss: 1.9428 - val_acc: 0.1650\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9294 - acc: 0.1775 - val_loss: 1.9257 - val_acc: 0.1770\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9096 - acc: 0.2039 - val_loss: 1.9090 - val_acc: 0.1930\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8883 - acc: 0.2343 - val_loss: 1.8896 - val_acc: 0.2300\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8639 - acc: 0.2639 - val_loss: 1.8667 - val_acc: 0.2600\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8362 - acc: 0.2941 - val_loss: 1.8406 - val_acc: 0.2930\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8047 - acc: 0.3197 - val_loss: 1.8104 - val_acc: 0.3130\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7697 - acc: 0.3525 - val_loss: 1.7769 - val_acc: 0.3270\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7306 - acc: 0.3848 - val_loss: 1.7381 - val_acc: 0.3570\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6877 - acc: 0.4065 - val_loss: 1.6967 - val_acc: 0.4010\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6420 - acc: 0.4327 - val_loss: 1.6534 - val_acc: 0.4190\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5942 - acc: 0.4536 - val_loss: 1.6077 - val_acc: 0.4420\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5453 - acc: 0.4763 - val_loss: 1.5613 - val_acc: 0.4650\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4962 - acc: 0.5036 - val_loss: 1.5170 - val_acc: 0.4780\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4475 - acc: 0.5193 - val_loss: 1.4734 - val_acc: 0.5090\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4001 - acc: 0.5443 - val_loss: 1.4309 - val_acc: 0.5200\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3540 - acc: 0.5683 - val_loss: 1.3883 - val_acc: 0.5510\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3097 - acc: 0.5856 - val_loss: 1.3493 - val_acc: 0.5760\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2662 - acc: 0.6064 - val_loss: 1.3126 - val_acc: 0.5750\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2257 - acc: 0.6281 - val_loss: 1.2727 - val_acc: 0.5790\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1865 - acc: 0.6372 - val_loss: 1.2380 - val_acc: 0.6010\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1491 - acc: 0.6543 - val_loss: 1.2037 - val_acc: 0.6100\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1134 - acc: 0.6635 - val_loss: 1.1715 - val_acc: 0.6170\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0794 - acc: 0.6735 - val_loss: 1.1423 - val_acc: 0.6220\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0473 - acc: 0.6812 - val_loss: 1.1129 - val_acc: 0.6310\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0160 - acc: 0.6903 - val_loss: 1.0861 - val_acc: 0.6440\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9873 - acc: 0.6987 - val_loss: 1.0618 - val_acc: 0.6550\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9597 - acc: 0.7032 - val_loss: 1.0376 - val_acc: 0.6610\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9336 - acc: 0.7105 - val_loss: 1.0124 - val_acc: 0.6760\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9091 - acc: 0.7169 - val_loss: 0.9924 - val_acc: 0.6740\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8862 - acc: 0.7216 - val_loss: 0.9706 - val_acc: 0.6840\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8647 - acc: 0.7257 - val_loss: 0.9521 - val_acc: 0.6890\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8442 - acc: 0.7308 - val_loss: 0.9328 - val_acc: 0.6960\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8249 - acc: 0.7336 - val_loss: 0.9164 - val_acc: 0.7020\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8066 - acc: 0.7388 - val_loss: 0.9036 - val_acc: 0.7010\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7898 - acc: 0.7433 - val_loss: 0.8874 - val_acc: 0.7050\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7738 - acc: 0.7464 - val_loss: 0.8728 - val_acc: 0.7120\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7585 - acc: 0.7487 - val_loss: 0.8595 - val_acc: 0.7110\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7445 - acc: 0.7539 - val_loss: 0.8487 - val_acc: 0.7110\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7313 - acc: 0.7564 - val_loss: 0.8396 - val_acc: 0.7100\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7184 - acc: 0.7609 - val_loss: 0.8294 - val_acc: 0.7110\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7062 - acc: 0.7627 - val_loss: 0.8167 - val_acc: 0.7160\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6948 - acc: 0.7669 - val_loss: 0.8067 - val_acc: 0.7240\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6838 - acc: 0.7684 - val_loss: 0.8006 - val_acc: 0.7210\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6736 - acc: 0.7707 - val_loss: 0.7906 - val_acc: 0.7220\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6635 - acc: 0.7748 - val_loss: 0.7828 - val_acc: 0.7280\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6540 - acc: 0.7776 - val_loss: 0.7805 - val_acc: 0.7250\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6450 - acc: 0.7803 - val_loss: 0.7693 - val_acc: 0.7290\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6364 - acc: 0.7811 - val_loss: 0.7628 - val_acc: 0.7310\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6278 - acc: 0.7843 - val_loss: 0.7562 - val_acc: 0.7320\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6202 - acc: 0.7872 - val_loss: 0.7519 - val_acc: 0.7320\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6127 - acc: 0.7880 - val_loss: 0.7441 - val_acc: 0.7280\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6051 - acc: 0.7903 - val_loss: 0.7437 - val_acc: 0.7300\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5978 - acc: 0.7924 - val_loss: 0.7351 - val_acc: 0.7300\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5908 - acc: 0.7957 - val_loss: 0.7291 - val_acc: 0.7350\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5846 - acc: 0.7945 - val_loss: 0.7239 - val_acc: 0.7420\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5779 - acc: 0.7968 - val_loss: 0.7223 - val_acc: 0.7430\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5716 - acc: 0.7996 - val_loss: 0.7173 - val_acc: 0.7420\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5655 - acc: 0.8031 - val_loss: 0.7166 - val_acc: 0.7380\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5602 - acc: 0.8017 - val_loss: 0.7125 - val_acc: 0.7420\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5544 - acc: 0.8045 - val_loss: 0.7079 - val_acc: 0.7450\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5483 - acc: 0.8069 - val_loss: 0.7040 - val_acc: 0.7470\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5432 - acc: 0.8075 - val_loss: 0.7018 - val_acc: 0.7440\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5379 - acc: 0.8091 - val_loss: 0.6975 - val_acc: 0.7470\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5327 - acc: 0.8111 - val_loss: 0.6952 - val_acc: 0.7450\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5273 - acc: 0.8143 - val_loss: 0.6954 - val_acc: 0.7420\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5229 - acc: 0.8172 - val_loss: 0.6909 - val_acc: 0.7440\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5178 - acc: 0.8152 - val_loss: 0.6859 - val_acc: 0.7520\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5132 - acc: 0.8181 - val_loss: 0.6837 - val_acc: 0.7520\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5090 - acc: 0.8220 - val_loss: 0.6810 - val_acc: 0.7540\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5046 - acc: 0.8224 - val_loss: 0.6839 - val_acc: 0.7530\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4999 - acc: 0.8212 - val_loss: 0.6766 - val_acc: 0.7570\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4956 - acc: 0.8256 - val_loss: 0.6800 - val_acc: 0.7520\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4915 - acc: 0.8292 - val_loss: 0.6745 - val_acc: 0.7550\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4874 - acc: 0.8287 - val_loss: 0.6743 - val_acc: 0.7530\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4832 - acc: 0.8301 - val_loss: 0.6759 - val_acc: 0.7560\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4793 - acc: 0.8336 - val_loss: 0.6690 - val_acc: 0.7590\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4753 - acc: 0.8332 - val_loss: 0.6681 - val_acc: 0.7540\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4713 - acc: 0.8353 - val_loss: 0.6670 - val_acc: 0.7700\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4679 - acc: 0.8385 - val_loss: 0.6659 - val_acc: 0.7610\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.4634 - acc: 0.8388 - val_loss: 0.6659 - val_acc: 0.7600\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4606 - acc: 0.8411 - val_loss: 0.6643 - val_acc: 0.7560\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4566 - acc: 0.8423 - val_loss: 0.6641 - val_acc: 0.7660\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4529 - acc: 0.8425 - val_loss: 0.6610 - val_acc: 0.7600\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4492 - acc: 0.8439 - val_loss: 0.6639 - val_acc: 0.7700\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4461 - acc: 0.8459 - val_loss: 0.6614 - val_acc: 0.7660\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4426 - acc: 0.8464 - val_loss: 0.6594 - val_acc: 0.7640\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4390 - acc: 0.8495 - val_loss: 0.6548 - val_acc: 0.7750\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4359 - acc: 0.8520 - val_loss: 0.6563 - val_acc: 0.7720\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4328 - acc: 0.8515 - val_loss: 0.6547 - val_acc: 0.7720\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4294 - acc: 0.8517 - val_loss: 0.6549 - val_acc: 0.7740\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4260 - acc: 0.8537 - val_loss: 0.6530 - val_acc: 0.7730\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4230 - acc: 0.8559 - val_loss: 0.6544 - val_acc: 0.7710\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4200 - acc: 0.8569 - val_loss: 0.6550 - val_acc: 0.7660\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4170 - acc: 0.8577 - val_loss: 0.6568 - val_acc: 0.7690\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4136 - acc: 0.8600 - val_loss: 0.6551 - val_acc: 0.7730\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4113 - acc: 0.8600 - val_loss: 0.6509 - val_acc: 0.7770\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4079 - acc: 0.8621 - val_loss: 0.6517 - val_acc: 0.7690\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4051 - acc: 0.8617 - val_loss: 0.6542 - val_acc: 0.7690\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4023 - acc: 0.8629 - val_loss: 0.6526 - val_acc: 0.7700\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3996 - acc: 0.8649 - val_loss: 0.6597 - val_acc: 0.7670\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3962 - acc: 0.8651 - val_loss: 0.6517 - val_acc: 0.7700\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3936 - acc: 0.8676 - val_loss: 0.6554 - val_acc: 0.7750\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3912 - acc: 0.8669 - val_loss: 0.6484 - val_acc: 0.7750\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3877 - acc: 0.8695 - val_loss: 0.6525 - val_acc: 0.7670\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3853 - acc: 0.8715 - val_loss: 0.6499 - val_acc: 0.7700\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3831 - acc: 0.8725 - val_loss: 0.6511 - val_acc: 0.7700\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3802 - acc: 0.8720 - val_loss: 0.6491 - val_acc: 0.7720\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3773 - acc: 0.8743 - val_loss: 0.6497 - val_acc: 0.7710\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3751 - acc: 0.8737 - val_loss: 0.6475 - val_acc: 0.7720\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3726 - acc: 0.8753 - val_loss: 0.6478 - val_acc: 0.7720\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3697 - acc: 0.8760 - val_loss: 0.6543 - val_acc: 0.7710\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3673 - acc: 0.8777 - val_loss: 0.6478 - val_acc: 0.7720\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.3649 - acc: 0.8791 - val_loss: 0.6516 - val_acc: 0.7750\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.3626 - acc: 0.8801 - val_loss: 0.6477 - val_acc: 0.7740\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3600 - acc: 0.8807 - val_loss: 0.6497 - val_acc: 0.7750\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3577 - acc: 0.8820 - val_loss: 0.6475 - val_acc: 0.7770\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3550 - acc: 0.8857 - val_loss: 0.6488 - val_acc: 0.7770\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3526 - acc: 0.8833 - val_loss: 0.6502 - val_acc: 0.7740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3505 - acc: 0.8861 - val_loss: 0.6531 - val_acc: 0.7730\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34799625634352366, 0.8847999999682109]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6552248989741007, 0.7693333336512248]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9468 - acc: 0.1477 - val_loss: 1.9306 - val_acc: 0.1700\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9166 - acc: 0.1761 - val_loss: 1.9073 - val_acc: 0.1850\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8918 - acc: 0.2131 - val_loss: 1.8848 - val_acc: 0.2210\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8664 - acc: 0.2488 - val_loss: 1.8606 - val_acc: 0.2460\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8385 - acc: 0.2812 - val_loss: 1.8334 - val_acc: 0.2680\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8065 - acc: 0.3019 - val_loss: 1.8019 - val_acc: 0.2980\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7703 - acc: 0.3259 - val_loss: 1.7662 - val_acc: 0.3160\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7296 - acc: 0.3519 - val_loss: 1.7267 - val_acc: 0.3340\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6839 - acc: 0.3771 - val_loss: 1.6834 - val_acc: 0.3570\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6350 - acc: 0.4028 - val_loss: 1.6361 - val_acc: 0.3850\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5830 - acc: 0.4320 - val_loss: 1.5871 - val_acc: 0.4210\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5298 - acc: 0.4624 - val_loss: 1.5378 - val_acc: 0.4470\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4755 - acc: 0.4913 - val_loss: 1.4888 - val_acc: 0.4640\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4213 - acc: 0.5231 - val_loss: 1.4401 - val_acc: 0.5000\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3683 - acc: 0.5496 - val_loss: 1.3931 - val_acc: 0.5150\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3166 - acc: 0.5759 - val_loss: 1.3457 - val_acc: 0.5350\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2675 - acc: 0.5961 - val_loss: 1.3021 - val_acc: 0.5620\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2205 - acc: 0.6092 - val_loss: 1.2612 - val_acc: 0.5710\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1760 - acc: 0.6257 - val_loss: 1.2199 - val_acc: 0.5770\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1344 - acc: 0.6393 - val_loss: 1.1821 - val_acc: 0.5910\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0955 - acc: 0.6501 - val_loss: 1.1485 - val_acc: 0.5980\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0596 - acc: 0.6588 - val_loss: 1.1164 - val_acc: 0.6130\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0259 - acc: 0.6685 - val_loss: 1.0872 - val_acc: 0.6210\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9951 - acc: 0.6749 - val_loss: 1.0594 - val_acc: 0.6350\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9663 - acc: 0.6863 - val_loss: 1.0377 - val_acc: 0.6290\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9401 - acc: 0.6944 - val_loss: 1.0115 - val_acc: 0.6420\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9149 - acc: 0.7007 - val_loss: 0.9920 - val_acc: 0.6470\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8920 - acc: 0.7060 - val_loss: 0.9694 - val_acc: 0.6560\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8706 - acc: 0.7119 - val_loss: 0.9514 - val_acc: 0.6580\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8509 - acc: 0.7189 - val_loss: 0.9343 - val_acc: 0.6630\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8323 - acc: 0.7221 - val_loss: 0.9209 - val_acc: 0.6730\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8146 - acc: 0.7305 - val_loss: 0.9021 - val_acc: 0.6770\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7988 - acc: 0.7312 - val_loss: 0.8894 - val_acc: 0.6830\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7831 - acc: 0.7360 - val_loss: 0.8795 - val_acc: 0.6850\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7691 - acc: 0.7387 - val_loss: 0.8635 - val_acc: 0.6810\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7553 - acc: 0.7441 - val_loss: 0.8551 - val_acc: 0.6920\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7422 - acc: 0.7493 - val_loss: 0.8428 - val_acc: 0.6860\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7301 - acc: 0.7525 - val_loss: 0.8329 - val_acc: 0.6830\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7185 - acc: 0.7548 - val_loss: 0.8232 - val_acc: 0.6980\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7073 - acc: 0.7577 - val_loss: 0.8164 - val_acc: 0.6950\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6968 - acc: 0.7609 - val_loss: 0.8053 - val_acc: 0.6930\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6874 - acc: 0.7659 - val_loss: 0.8041 - val_acc: 0.6980\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6773 - acc: 0.7695 - val_loss: 0.7952 - val_acc: 0.7030\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6683 - acc: 0.7719 - val_loss: 0.7876 - val_acc: 0.7080\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6591 - acc: 0.7745 - val_loss: 0.7779 - val_acc: 0.7060\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6505 - acc: 0.7783 - val_loss: 0.7719 - val_acc: 0.7180\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6421 - acc: 0.7807 - val_loss: 0.7704 - val_acc: 0.7180\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6346 - acc: 0.7820 - val_loss: 0.7597 - val_acc: 0.7170\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6266 - acc: 0.7845 - val_loss: 0.7540 - val_acc: 0.7180\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6193 - acc: 0.7860 - val_loss: 0.7521 - val_acc: 0.7230\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6123 - acc: 0.7883 - val_loss: 0.7453 - val_acc: 0.7280\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6054 - acc: 0.7900 - val_loss: 0.7409 - val_acc: 0.7310\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5986 - acc: 0.7924 - val_loss: 0.7360 - val_acc: 0.7260\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5921 - acc: 0.7952 - val_loss: 0.7303 - val_acc: 0.7330\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5855 - acc: 0.7972 - val_loss: 0.7257 - val_acc: 0.7350\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5795 - acc: 0.7996 - val_loss: 0.7219 - val_acc: 0.7300\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5739 - acc: 0.8016 - val_loss: 0.7176 - val_acc: 0.7340\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5681 - acc: 0.8016 - val_loss: 0.7143 - val_acc: 0.7350\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.5618 - acc: 0.8051 - val_loss: 0.7133 - val_acc: 0.7340\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.5564 - acc: 0.8073 - val_loss: 0.7100 - val_acc: 0.7380\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 25us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.55189603861173, 0.8078666666984559]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.706461408774058, 0.7399999998410542]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 2.5957 - acc: 0.1609 - val_loss: 2.5780 - val_acc: 0.1820\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.5665 - acc: 0.1819 - val_loss: 2.5554 - val_acc: 0.2170\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.5414 - acc: 0.2097 - val_loss: 2.5335 - val_acc: 0.2330\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.5156 - acc: 0.2343 - val_loss: 2.5109 - val_acc: 0.2450\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.4878 - acc: 0.2652 - val_loss: 2.4861 - val_acc: 0.2660\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.4575 - acc: 0.3017 - val_loss: 2.4583 - val_acc: 0.2910\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.4241 - acc: 0.3345 - val_loss: 2.4266 - val_acc: 0.3240\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.3875 - acc: 0.3683 - val_loss: 2.3926 - val_acc: 0.3540\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.3478 - acc: 0.3961 - val_loss: 2.3561 - val_acc: 0.3730\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.3059 - acc: 0.4267 - val_loss: 2.3167 - val_acc: 0.3970\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.2617 - acc: 0.4547 - val_loss: 2.2751 - val_acc: 0.4370\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.2157 - acc: 0.4847 - val_loss: 2.2317 - val_acc: 0.4680\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.1671 - acc: 0.5036 - val_loss: 2.1865 - val_acc: 0.4890\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.1170 - acc: 0.5277 - val_loss: 2.1367 - val_acc: 0.5090\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.0645 - acc: 0.5545 - val_loss: 2.0880 - val_acc: 0.5200\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.0118 - acc: 0.5753 - val_loss: 2.0375 - val_acc: 0.5360\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9601 - acc: 0.5936 - val_loss: 1.9895 - val_acc: 0.5600\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9090 - acc: 0.6127 - val_loss: 1.9424 - val_acc: 0.5770\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8604 - acc: 0.6295 - val_loss: 1.8964 - val_acc: 0.5860\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8129 - acc: 0.6424 - val_loss: 1.8517 - val_acc: 0.6100\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7679 - acc: 0.6580 - val_loss: 1.8122 - val_acc: 0.6220\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7259 - acc: 0.6731 - val_loss: 1.7739 - val_acc: 0.6300\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6870 - acc: 0.6789 - val_loss: 1.7383 - val_acc: 0.6480\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6505 - acc: 0.6893 - val_loss: 1.7055 - val_acc: 0.6510\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6167 - acc: 0.6971 - val_loss: 1.6730 - val_acc: 0.6710\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5847 - acc: 0.7037 - val_loss: 1.6460 - val_acc: 0.6740\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5551 - acc: 0.7135 - val_loss: 1.6198 - val_acc: 0.6810\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5269 - acc: 0.7199 - val_loss: 1.5933 - val_acc: 0.6900\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5013 - acc: 0.7269 - val_loss: 1.5693 - val_acc: 0.6930\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4770 - acc: 0.7292 - val_loss: 1.5476 - val_acc: 0.6900\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4540 - acc: 0.7341 - val_loss: 1.5276 - val_acc: 0.6970\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4326 - acc: 0.7408 - val_loss: 1.5084 - val_acc: 0.7010\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4121 - acc: 0.7455 - val_loss: 1.4910 - val_acc: 0.7030\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3929 - acc: 0.7484 - val_loss: 1.4744 - val_acc: 0.7090\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3747 - acc: 0.7536 - val_loss: 1.4585 - val_acc: 0.7060\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3577 - acc: 0.7572 - val_loss: 1.4438 - val_acc: 0.7120\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3416 - acc: 0.7591 - val_loss: 1.4301 - val_acc: 0.7150\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3262 - acc: 0.7647 - val_loss: 1.4168 - val_acc: 0.7160\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3116 - acc: 0.7667 - val_loss: 1.4051 - val_acc: 0.7180\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2981 - acc: 0.7708 - val_loss: 1.3948 - val_acc: 0.7200\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2849 - acc: 0.7732 - val_loss: 1.3833 - val_acc: 0.7230\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2720 - acc: 0.7757 - val_loss: 1.3731 - val_acc: 0.7310\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2599 - acc: 0.7803 - val_loss: 1.3636 - val_acc: 0.7240\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2485 - acc: 0.7820 - val_loss: 1.3556 - val_acc: 0.7260\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2374 - acc: 0.7832 - val_loss: 1.3449 - val_acc: 0.7340\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2265 - acc: 0.7881 - val_loss: 1.3349 - val_acc: 0.7350\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2163 - acc: 0.7889 - val_loss: 1.3272 - val_acc: 0.7360\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2062 - acc: 0.7913 - val_loss: 1.3206 - val_acc: 0.7330\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1969 - acc: 0.7937 - val_loss: 1.3120 - val_acc: 0.7400\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1872 - acc: 0.7959 - val_loss: 1.3089 - val_acc: 0.7340\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1784 - acc: 0.7972 - val_loss: 1.3010 - val_acc: 0.7400\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1697 - acc: 0.7997 - val_loss: 1.2928 - val_acc: 0.7430\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1613 - acc: 0.8017 - val_loss: 1.2846 - val_acc: 0.7460\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1531 - acc: 0.8023 - val_loss: 1.2802 - val_acc: 0.7480\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1452 - acc: 0.8048 - val_loss: 1.2726 - val_acc: 0.7480\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1375 - acc: 0.8060 - val_loss: 1.2685 - val_acc: 0.7470\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1297 - acc: 0.8081 - val_loss: 1.2629 - val_acc: 0.7530\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1223 - acc: 0.8117 - val_loss: 1.2569 - val_acc: 0.7550\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1148 - acc: 0.8113 - val_loss: 1.2525 - val_acc: 0.7530\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1080 - acc: 0.8125 - val_loss: 1.2494 - val_acc: 0.7510\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1013 - acc: 0.8140 - val_loss: 1.2424 - val_acc: 0.7520\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0940 - acc: 0.8159 - val_loss: 1.2363 - val_acc: 0.7620\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0877 - acc: 0.8171 - val_loss: 1.2310 - val_acc: 0.7580\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0812 - acc: 0.8176 - val_loss: 1.2276 - val_acc: 0.7620\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0747 - acc: 0.8193 - val_loss: 1.2235 - val_acc: 0.7590\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0683 - acc: 0.8229 - val_loss: 1.2204 - val_acc: 0.7620\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0621 - acc: 0.8241 - val_loss: 1.2168 - val_acc: 0.7610\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0562 - acc: 0.8259 - val_loss: 1.2126 - val_acc: 0.7640\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0500 - acc: 0.8284 - val_loss: 1.2055 - val_acc: 0.7610\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0448 - acc: 0.8263 - val_loss: 1.2031 - val_acc: 0.7710\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0387 - acc: 0.8307 - val_loss: 1.2022 - val_acc: 0.7550\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0334 - acc: 0.8305 - val_loss: 1.1963 - val_acc: 0.7620\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0276 - acc: 0.8319 - val_loss: 1.1930 - val_acc: 0.7640\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0223 - acc: 0.8319 - val_loss: 1.1891 - val_acc: 0.7610\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0168 - acc: 0.8344 - val_loss: 1.1848 - val_acc: 0.7650\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0112 - acc: 0.8357 - val_loss: 1.1830 - val_acc: 0.7750\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0065 - acc: 0.8367 - val_loss: 1.1795 - val_acc: 0.7670\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0011 - acc: 0.8365 - val_loss: 1.1761 - val_acc: 0.7640\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9962 - acc: 0.8399 - val_loss: 1.1739 - val_acc: 0.7660\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9913 - acc: 0.8413 - val_loss: 1.1705 - val_acc: 0.7690\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9861 - acc: 0.8425 - val_loss: 1.1663 - val_acc: 0.7720\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9811 - acc: 0.8428 - val_loss: 1.1673 - val_acc: 0.7660\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9763 - acc: 0.8468 - val_loss: 1.1628 - val_acc: 0.7650\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9719 - acc: 0.8447 - val_loss: 1.1575 - val_acc: 0.7720\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9671 - acc: 0.8456 - val_loss: 1.1543 - val_acc: 0.7770\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9623 - acc: 0.8487 - val_loss: 1.1537 - val_acc: 0.7730\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9575 - acc: 0.8500 - val_loss: 1.1520 - val_acc: 0.7720\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9530 - acc: 0.8499 - val_loss: 1.1479 - val_acc: 0.7690\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9486 - acc: 0.8519 - val_loss: 1.1495 - val_acc: 0.7690\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9439 - acc: 0.8520 - val_loss: 1.1440 - val_acc: 0.7690\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9399 - acc: 0.8541 - val_loss: 1.1409 - val_acc: 0.7800\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9351 - acc: 0.8551 - val_loss: 1.1383 - val_acc: 0.7730\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9307 - acc: 0.8592 - val_loss: 1.1368 - val_acc: 0.7730\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9267 - acc: 0.8551 - val_loss: 1.1349 - val_acc: 0.7700\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9227 - acc: 0.8575 - val_loss: 1.1301 - val_acc: 0.7760\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9183 - acc: 0.8579 - val_loss: 1.1275 - val_acc: 0.7770\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9143 - acc: 0.8625 - val_loss: 1.1252 - val_acc: 0.7780\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9100 - acc: 0.8604 - val_loss: 1.1249 - val_acc: 0.7810\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9060 - acc: 0.8629 - val_loss: 1.1221 - val_acc: 0.7700\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9018 - acc: 0.8637 - val_loss: 1.1202 - val_acc: 0.7770\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8980 - acc: 0.8633 - val_loss: 1.1167 - val_acc: 0.7810\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8943 - acc: 0.8655 - val_loss: 1.1150 - val_acc: 0.7770\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8906 - acc: 0.8653 - val_loss: 1.1138 - val_acc: 0.7750\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8863 - acc: 0.8663 - val_loss: 1.1166 - val_acc: 0.7760\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8827 - acc: 0.8683 - val_loss: 1.1088 - val_acc: 0.7720\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8786 - acc: 0.8699 - val_loss: 1.1068 - val_acc: 0.7710\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8749 - acc: 0.8703 - val_loss: 1.1048 - val_acc: 0.7720\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8710 - acc: 0.8733 - val_loss: 1.1068 - val_acc: 0.7770\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8676 - acc: 0.8740 - val_loss: 1.1036 - val_acc: 0.7770\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8643 - acc: 0.8735 - val_loss: 1.1002 - val_acc: 0.7740\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8602 - acc: 0.8741 - val_loss: 1.0978 - val_acc: 0.7810\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8566 - acc: 0.8760 - val_loss: 1.0971 - val_acc: 0.7790\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8532 - acc: 0.8760 - val_loss: 1.0948 - val_acc: 0.7760\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8496 - acc: 0.8772 - val_loss: 1.0909 - val_acc: 0.7780\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8464 - acc: 0.8791 - val_loss: 1.0901 - val_acc: 0.7740\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8427 - acc: 0.8797 - val_loss: 1.0887 - val_acc: 0.7790\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8394 - acc: 0.8789 - val_loss: 1.0862 - val_acc: 0.7800\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8364 - acc: 0.8811 - val_loss: 1.0891 - val_acc: 0.7750\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8329 - acc: 0.8815 - val_loss: 1.0854 - val_acc: 0.7780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8293 - acc: 0.8832 - val_loss: 1.0825 - val_acc: 0.7780\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 15.9907 - acc: 0.1697 - val_loss: 15.5828 - val_acc: 0.1830\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 15.2283 - acc: 0.1959 - val_loss: 14.8401 - val_acc: 0.1910\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 14.4952 - acc: 0.2061 - val_loss: 14.1213 - val_acc: 0.2050\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 13.7850 - acc: 0.2123 - val_loss: 13.4235 - val_acc: 0.2040\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 13.0953 - acc: 0.2244 - val_loss: 12.7461 - val_acc: 0.2120\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 12.4249 - acc: 0.2392 - val_loss: 12.0870 - val_acc: 0.2420\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 11.7729 - acc: 0.2616 - val_loss: 11.4468 - val_acc: 0.2660\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 11.1388 - acc: 0.2933 - val_loss: 10.8244 - val_acc: 0.2890\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 10.5231 - acc: 0.3209 - val_loss: 10.2208 - val_acc: 0.3080\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 9.9251 - acc: 0.3479 - val_loss: 9.6346 - val_acc: 0.3460\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 9.3451 - acc: 0.3759 - val_loss: 9.0679 - val_acc: 0.3810\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 8.7841 - acc: 0.4107 - val_loss: 8.5217 - val_acc: 0.4020\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 8.2451 - acc: 0.4399 - val_loss: 7.9977 - val_acc: 0.4250\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 7.7292 - acc: 0.4628 - val_loss: 7.4968 - val_acc: 0.4570\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 7.2364 - acc: 0.4836 - val_loss: 7.0188 - val_acc: 0.4790\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 6.7669 - acc: 0.5041 - val_loss: 6.5627 - val_acc: 0.4870\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 6.3205 - acc: 0.5237 - val_loss: 6.1311 - val_acc: 0.5090\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 5.8975 - acc: 0.5404 - val_loss: 5.7206 - val_acc: 0.5160\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 5.4979 - acc: 0.5503 - val_loss: 5.3358 - val_acc: 0.5260\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 5.1216 - acc: 0.5604 - val_loss: 4.9716 - val_acc: 0.5370\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 4.7686 - acc: 0.5685 - val_loss: 4.6312 - val_acc: 0.5480\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 4.4377 - acc: 0.5781 - val_loss: 4.3133 - val_acc: 0.5650\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 4.1298 - acc: 0.5895 - val_loss: 4.0185 - val_acc: 0.5680\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.8447 - acc: 0.5977 - val_loss: 3.7438 - val_acc: 0.5720\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 3.5820 - acc: 0.6031 - val_loss: 3.4930 - val_acc: 0.5860\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.3414 - acc: 0.6075 - val_loss: 3.2670 - val_acc: 0.5900\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.1233 - acc: 0.6163 - val_loss: 3.0563 - val_acc: 0.6000\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.9271 - acc: 0.6233 - val_loss: 2.8721 - val_acc: 0.6040\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.7523 - acc: 0.6271 - val_loss: 2.7073 - val_acc: 0.6140\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.5986 - acc: 0.6300 - val_loss: 2.5633 - val_acc: 0.6100\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.4653 - acc: 0.6331 - val_loss: 2.4399 - val_acc: 0.6290\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.3517 - acc: 0.6376 - val_loss: 2.3378 - val_acc: 0.6280\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.2579 - acc: 0.6421 - val_loss: 2.2504 - val_acc: 0.6290\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.1820 - acc: 0.6472 - val_loss: 2.1841 - val_acc: 0.6370\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.1234 - acc: 0.6504 - val_loss: 2.1319 - val_acc: 0.6410\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.0804 - acc: 0.6520 - val_loss: 2.0963 - val_acc: 0.6460\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.0487 - acc: 0.6529 - val_loss: 2.0704 - val_acc: 0.6430\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.0234 - acc: 0.6521 - val_loss: 2.0443 - val_acc: 0.6430\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.0013 - acc: 0.6575 - val_loss: 2.0244 - val_acc: 0.6420\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9813 - acc: 0.6585 - val_loss: 2.0043 - val_acc: 0.6420\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9628 - acc: 0.6635 - val_loss: 1.9861 - val_acc: 0.6430\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9459 - acc: 0.6639 - val_loss: 1.9672 - val_acc: 0.6380\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9295 - acc: 0.6661 - val_loss: 1.9528 - val_acc: 0.6490\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9138 - acc: 0.6693 - val_loss: 1.9360 - val_acc: 0.6510\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8990 - acc: 0.6689 - val_loss: 1.9244 - val_acc: 0.6450\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8856 - acc: 0.6704 - val_loss: 1.9112 - val_acc: 0.6560\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8710 - acc: 0.6729 - val_loss: 1.8954 - val_acc: 0.6550\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8578 - acc: 0.6740 - val_loss: 1.8847 - val_acc: 0.6600\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8451 - acc: 0.6773 - val_loss: 1.8694 - val_acc: 0.6590\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8325 - acc: 0.6749 - val_loss: 1.8682 - val_acc: 0.6570\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8209 - acc: 0.6771 - val_loss: 1.8432 - val_acc: 0.6620\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8084 - acc: 0.6768 - val_loss: 1.8306 - val_acc: 0.6590\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7966 - acc: 0.6777 - val_loss: 1.8226 - val_acc: 0.6640\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7856 - acc: 0.6787 - val_loss: 1.8196 - val_acc: 0.6670\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7755 - acc: 0.6811 - val_loss: 1.7964 - val_acc: 0.6660\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7641 - acc: 0.6808 - val_loss: 1.7864 - val_acc: 0.6640\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7538 - acc: 0.6804 - val_loss: 1.7768 - val_acc: 0.6650\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7432 - acc: 0.6839 - val_loss: 1.7658 - val_acc: 0.6640\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7333 - acc: 0.6840 - val_loss: 1.7577 - val_acc: 0.6690\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7238 - acc: 0.6841 - val_loss: 1.7448 - val_acc: 0.6670\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7136 - acc: 0.6847 - val_loss: 1.7419 - val_acc: 0.6750\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7043 - acc: 0.6852 - val_loss: 1.7285 - val_acc: 0.6700\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6948 - acc: 0.6863 - val_loss: 1.7177 - val_acc: 0.6720\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6857 - acc: 0.6868 - val_loss: 1.7088 - val_acc: 0.6670\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6764 - acc: 0.6877 - val_loss: 1.7064 - val_acc: 0.6710\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6677 - acc: 0.6861 - val_loss: 1.6942 - val_acc: 0.6740\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6586 - acc: 0.6895 - val_loss: 1.6835 - val_acc: 0.6740\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6495 - acc: 0.6897 - val_loss: 1.6753 - val_acc: 0.6760\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6412 - acc: 0.6908 - val_loss: 1.6657 - val_acc: 0.6770\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6328 - acc: 0.6908 - val_loss: 1.6555 - val_acc: 0.6750\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6249 - acc: 0.6896 - val_loss: 1.6505 - val_acc: 0.6800\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6161 - acc: 0.6917 - val_loss: 1.6389 - val_acc: 0.6810\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6083 - acc: 0.6923 - val_loss: 1.6298 - val_acc: 0.6770\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6006 - acc: 0.6916 - val_loss: 1.6213 - val_acc: 0.6810\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5930 - acc: 0.6933 - val_loss: 1.6182 - val_acc: 0.6830\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5851 - acc: 0.6943 - val_loss: 1.6100 - val_acc: 0.6840\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5769 - acc: 0.6936 - val_loss: 1.6022 - val_acc: 0.6860\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5695 - acc: 0.6952 - val_loss: 1.5964 - val_acc: 0.6820\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5623 - acc: 0.6957 - val_loss: 1.5864 - val_acc: 0.6830\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5541 - acc: 0.6961 - val_loss: 1.5864 - val_acc: 0.6810\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5473 - acc: 0.6964 - val_loss: 1.5692 - val_acc: 0.6830\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5398 - acc: 0.6956 - val_loss: 1.5621 - val_acc: 0.6860\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5329 - acc: 0.6961 - val_loss: 1.5539 - val_acc: 0.6840\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5263 - acc: 0.6977 - val_loss: 1.5531 - val_acc: 0.6830\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5194 - acc: 0.6980 - val_loss: 1.5414 - val_acc: 0.6850\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5130 - acc: 0.6985 - val_loss: 1.5342 - val_acc: 0.6900\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5056 - acc: 0.6976 - val_loss: 1.5293 - val_acc: 0.6890\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4990 - acc: 0.6984 - val_loss: 1.5216 - val_acc: 0.6860\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4925 - acc: 0.6993 - val_loss: 1.5168 - val_acc: 0.6880\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4860 - acc: 0.6988 - val_loss: 1.5131 - val_acc: 0.6840\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4796 - acc: 0.7001 - val_loss: 1.5049 - val_acc: 0.6890\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4731 - acc: 0.6997 - val_loss: 1.4937 - val_acc: 0.6910\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4666 - acc: 0.7005 - val_loss: 1.4885 - val_acc: 0.6900\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4606 - acc: 0.7021 - val_loss: 1.4819 - val_acc: 0.6920\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4537 - acc: 0.7016 - val_loss: 1.4797 - val_acc: 0.6900\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4480 - acc: 0.7033 - val_loss: 1.4747 - val_acc: 0.6890\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4419 - acc: 0.7036 - val_loss: 1.4751 - val_acc: 0.6940\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4366 - acc: 0.7035 - val_loss: 1.4620 - val_acc: 0.6900\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4305 - acc: 0.7035 - val_loss: 1.4558 - val_acc: 0.6920\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4238 - acc: 0.7061 - val_loss: 1.4523 - val_acc: 0.6870\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4184 - acc: 0.7071 - val_loss: 1.4534 - val_acc: 0.6880\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4123 - acc: 0.7063 - val_loss: 1.4375 - val_acc: 0.6950\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4068 - acc: 0.7053 - val_loss: 1.4298 - val_acc: 0.6950\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4007 - acc: 0.7071 - val_loss: 1.4236 - val_acc: 0.6960\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3951 - acc: 0.7073 - val_loss: 1.4198 - val_acc: 0.6940\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3894 - acc: 0.7060 - val_loss: 1.4130 - val_acc: 0.6940\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3841 - acc: 0.7085 - val_loss: 1.4070 - val_acc: 0.6950\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3783 - acc: 0.7088 - val_loss: 1.4056 - val_acc: 0.6920\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3734 - acc: 0.7084 - val_loss: 1.3990 - val_acc: 0.6930\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3675 - acc: 0.7103 - val_loss: 1.3914 - val_acc: 0.6970\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3621 - acc: 0.7103 - val_loss: 1.3906 - val_acc: 0.6920\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3571 - acc: 0.7107 - val_loss: 1.3816 - val_acc: 0.6980\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3517 - acc: 0.7113 - val_loss: 1.3763 - val_acc: 0.6980\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3470 - acc: 0.7129 - val_loss: 1.3735 - val_acc: 0.6950\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3420 - acc: 0.7131 - val_loss: 1.3669 - val_acc: 0.6970\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3367 - acc: 0.7133 - val_loss: 1.3640 - val_acc: 0.6980\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3314 - acc: 0.7133 - val_loss: 1.3569 - val_acc: 0.6930\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3268 - acc: 0.7124 - val_loss: 1.3509 - val_acc: 0.6970\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3217 - acc: 0.7139 - val_loss: 1.3482 - val_acc: 0.6940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3171 - acc: 0.7141 - val_loss: 1.3466 - val_acc: 0.6990\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 15.9842 - acc: 0.1463 - val_loss: 15.5738 - val_acc: 0.1550\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 15.2103 - acc: 0.1959 - val_loss: 14.8252 - val_acc: 0.1930\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 14.4679 - acc: 0.2367 - val_loss: 14.0969 - val_acc: 0.2270\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 13.7453 - acc: 0.2639 - val_loss: 13.3865 - val_acc: 0.2620\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 13.0405 - acc: 0.2859 - val_loss: 12.6929 - val_acc: 0.2820\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 12.3539 - acc: 0.3071 - val_loss: 12.0179 - val_acc: 0.3040\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 11.6865 - acc: 0.3311 - val_loss: 11.3631 - val_acc: 0.3270\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 11.0397 - acc: 0.3516 - val_loss: 10.7302 - val_acc: 0.3450\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 10.4151 - acc: 0.3713 - val_loss: 10.1206 - val_acc: 0.3680\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 9.8139 - acc: 0.3924 - val_loss: 9.5330 - val_acc: 0.3800\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 9.2354 - acc: 0.4108 - val_loss: 8.9692 - val_acc: 0.4040\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 8.6799 - acc: 0.4379 - val_loss: 8.4275 - val_acc: 0.4320\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 8.1477 - acc: 0.4625 - val_loss: 7.9091 - val_acc: 0.4460\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 7.6386 - acc: 0.4792 - val_loss: 7.4147 - val_acc: 0.4740\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 7.1530 - acc: 0.5084 - val_loss: 6.9434 - val_acc: 0.4960\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 6.6905 - acc: 0.5293 - val_loss: 6.4953 - val_acc: 0.5150\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 6.2510 - acc: 0.5501 - val_loss: 6.0673 - val_acc: 0.5210\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 5.8348 - acc: 0.5752 - val_loss: 5.6653 - val_acc: 0.5330\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 5.4424 - acc: 0.5785 - val_loss: 5.2859 - val_acc: 0.5470\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 5.0740 - acc: 0.5937 - val_loss: 4.9316 - val_acc: 0.5600\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 4.7280 - acc: 0.6057 - val_loss: 4.5991 - val_acc: 0.5700\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 4.4057 - acc: 0.6044 - val_loss: 4.2884 - val_acc: 0.5790\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 4.1061 - acc: 0.6177 - val_loss: 4.0008 - val_acc: 0.5890\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.8287 - acc: 0.6236 - val_loss: 3.7363 - val_acc: 0.6120\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.5736 - acc: 0.6281 - val_loss: 3.4917 - val_acc: 0.6210\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.3395 - acc: 0.6335 - val_loss: 3.2705 - val_acc: 0.6140\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 3.1275 - acc: 0.6349 - val_loss: 3.0669 - val_acc: 0.6060\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.9359 - acc: 0.6375 - val_loss: 2.8867 - val_acc: 0.6290\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.7650 - acc: 0.6425 - val_loss: 2.7280 - val_acc: 0.6280\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.6152 - acc: 0.6423 - val_loss: 2.5852 - val_acc: 0.6330\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.4854 - acc: 0.6448 - val_loss: 2.4670 - val_acc: 0.6360\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.3750 - acc: 0.6473 - val_loss: 2.3647 - val_acc: 0.6270\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.2833 - acc: 0.6467 - val_loss: 2.2805 - val_acc: 0.6350\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.2093 - acc: 0.6508 - val_loss: 2.2158 - val_acc: 0.6320\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.1514 - acc: 0.6508 - val_loss: 2.1663 - val_acc: 0.6430\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.1082 - acc: 0.6540 - val_loss: 2.1266 - val_acc: 0.6410\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.0758 - acc: 0.6524 - val_loss: 2.0995 - val_acc: 0.6360\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.0496 - acc: 0.6528 - val_loss: 2.0745 - val_acc: 0.6430\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.0267 - acc: 0.6548 - val_loss: 2.0503 - val_acc: 0.6420\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.0061 - acc: 0.6561 - val_loss: 2.0314 - val_acc: 0.6440\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9866 - acc: 0.6609 - val_loss: 2.0111 - val_acc: 0.6440\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9687 - acc: 0.6612 - val_loss: 1.9936 - val_acc: 0.6480\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9517 - acc: 0.6635 - val_loss: 1.9766 - val_acc: 0.6490\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9356 - acc: 0.6637 - val_loss: 1.9608 - val_acc: 0.6510\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9200 - acc: 0.6669 - val_loss: 1.9431 - val_acc: 0.6530\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9052 - acc: 0.6659 - val_loss: 1.9307 - val_acc: 0.6550\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8909 - acc: 0.6683 - val_loss: 1.9138 - val_acc: 0.6530\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8767 - acc: 0.6705 - val_loss: 1.9020 - val_acc: 0.6470\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8640 - acc: 0.6687 - val_loss: 1.8865 - val_acc: 0.6520\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8508 - acc: 0.6709 - val_loss: 1.8724 - val_acc: 0.6600\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8381 - acc: 0.6707 - val_loss: 1.8599 - val_acc: 0.6550\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8259 - acc: 0.6713 - val_loss: 1.8470 - val_acc: 0.6640\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8141 - acc: 0.6727 - val_loss: 1.8383 - val_acc: 0.6560\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8022 - acc: 0.6743 - val_loss: 1.8305 - val_acc: 0.6650\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7911 - acc: 0.6753 - val_loss: 1.8143 - val_acc: 0.6670\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7796 - acc: 0.6761 - val_loss: 1.7997 - val_acc: 0.6640\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7691 - acc: 0.6777 - val_loss: 1.7927 - val_acc: 0.6640\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7586 - acc: 0.6777 - val_loss: 1.7807 - val_acc: 0.6660\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7483 - acc: 0.6804 - val_loss: 1.7663 - val_acc: 0.6690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7381 - acc: 0.6787 - val_loss: 1.7561 - val_acc: 0.6710\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7284 - acc: 0.6815 - val_loss: 1.7458 - val_acc: 0.6680\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7183 - acc: 0.6811 - val_loss: 1.7367 - val_acc: 0.6710\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7088 - acc: 0.6839 - val_loss: 1.7406 - val_acc: 0.6760\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7003 - acc: 0.6853 - val_loss: 1.7196 - val_acc: 0.6750\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6903 - acc: 0.6865 - val_loss: 1.7061 - val_acc: 0.6710\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6812 - acc: 0.6848 - val_loss: 1.6978 - val_acc: 0.6720\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6722 - acc: 0.6872 - val_loss: 1.6905 - val_acc: 0.6820\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6631 - acc: 0.6892 - val_loss: 1.6827 - val_acc: 0.6800\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6550 - acc: 0.6883 - val_loss: 1.6730 - val_acc: 0.6830\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6460 - acc: 0.6876 - val_loss: 1.6639 - val_acc: 0.6870\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6377 - acc: 0.6919 - val_loss: 1.6518 - val_acc: 0.6880\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6294 - acc: 0.6917 - val_loss: 1.6536 - val_acc: 0.6800\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6213 - acc: 0.6921 - val_loss: 1.6360 - val_acc: 0.6850\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6133 - acc: 0.6908 - val_loss: 1.6308 - val_acc: 0.6790\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6058 - acc: 0.6927 - val_loss: 1.6211 - val_acc: 0.6810\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5977 - acc: 0.6936 - val_loss: 1.6176 - val_acc: 0.6880\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5899 - acc: 0.6945 - val_loss: 1.6057 - val_acc: 0.6840\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5823 - acc: 0.6972 - val_loss: 1.5959 - val_acc: 0.6920\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5750 - acc: 0.6955 - val_loss: 1.5921 - val_acc: 0.6890\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5676 - acc: 0.6945 - val_loss: 1.5828 - val_acc: 0.6890\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5601 - acc: 0.6980 - val_loss: 1.5752 - val_acc: 0.6950\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5531 - acc: 0.6980 - val_loss: 1.5683 - val_acc: 0.6920\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5463 - acc: 0.6989 - val_loss: 1.5644 - val_acc: 0.6920\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5391 - acc: 0.7005 - val_loss: 1.5570 - val_acc: 0.6910\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5319 - acc: 0.6993 - val_loss: 1.5512 - val_acc: 0.6930\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5251 - acc: 0.6995 - val_loss: 1.5375 - val_acc: 0.6900\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5176 - acc: 0.7009 - val_loss: 1.5321 - val_acc: 0.6930\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5111 - acc: 0.7012 - val_loss: 1.5249 - val_acc: 0.6950\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5042 - acc: 0.7020 - val_loss: 1.5196 - val_acc: 0.6900\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4979 - acc: 0.7025 - val_loss: 1.5108 - val_acc: 0.6920\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4915 - acc: 0.7032 - val_loss: 1.5082 - val_acc: 0.6960\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4852 - acc: 0.7029 - val_loss: 1.4961 - val_acc: 0.6950\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4784 - acc: 0.7045 - val_loss: 1.4906 - val_acc: 0.6950\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4725 - acc: 0.7048 - val_loss: 1.4853 - val_acc: 0.6980\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4658 - acc: 0.7045 - val_loss: 1.4796 - val_acc: 0.6950\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4596 - acc: 0.7077 - val_loss: 1.4725 - val_acc: 0.6970\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4535 - acc: 0.7081 - val_loss: 1.4712 - val_acc: 0.6920\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4476 - acc: 0.7063 - val_loss: 1.4642 - val_acc: 0.6970\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4416 - acc: 0.7073 - val_loss: 1.4550 - val_acc: 0.6940\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4358 - acc: 0.7089 - val_loss: 1.4450 - val_acc: 0.6980\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4301 - acc: 0.7085 - val_loss: 1.4412 - val_acc: 0.6970\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4241 - acc: 0.7089 - val_loss: 1.4383 - val_acc: 0.6910\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4190 - acc: 0.7088 - val_loss: 1.4298 - val_acc: 0.6980\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4125 - acc: 0.7109 - val_loss: 1.4300 - val_acc: 0.7010\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4072 - acc: 0.7115 - val_loss: 1.4225 - val_acc: 0.6960\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4015 - acc: 0.7124 - val_loss: 1.4161 - val_acc: 0.7000\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3964 - acc: 0.7117 - val_loss: 1.4091 - val_acc: 0.6970\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3904 - acc: 0.7132 - val_loss: 1.4109 - val_acc: 0.6990\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3848 - acc: 0.7129 - val_loss: 1.3966 - val_acc: 0.7000\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3800 - acc: 0.7121 - val_loss: 1.3917 - val_acc: 0.7010\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3746 - acc: 0.7135 - val_loss: 1.3849 - val_acc: 0.7040\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3687 - acc: 0.7136 - val_loss: 1.3841 - val_acc: 0.7020\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3640 - acc: 0.7148 - val_loss: 1.3960 - val_acc: 0.7030\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3597 - acc: 0.7147 - val_loss: 1.3684 - val_acc: 0.7040\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3536 - acc: 0.7167 - val_loss: 1.3677 - val_acc: 0.6990\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3482 - acc: 0.7144 - val_loss: 1.3612 - val_acc: 0.7030\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3436 - acc: 0.7171 - val_loss: 1.3534 - val_acc: 0.7030\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3383 - acc: 0.7153 - val_loss: 1.3510 - val_acc: 0.7020\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3336 - acc: 0.7181 - val_loss: 1.3452 - val_acc: 0.7060\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3284 - acc: 0.7155 - val_loss: 1.3456 - val_acc: 0.7030\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3244 - acc: 0.7179 - val_loss: 1.3358 - val_acc: 0.7050\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3190 - acc: 0.7181 - val_loss: 1.3398 - val_acc: 0.7050\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3143 - acc: 0.7196 - val_loss: 1.3245 - val_acc: 0.7100\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3097 - acc: 0.7185 - val_loss: 1.3317 - val_acc: 0.7030\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3048 - acc: 0.7195 - val_loss: 1.3259 - val_acc: 0.7010\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3005 - acc: 0.7204 - val_loss: 1.3122 - val_acc: 0.7040\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2963 - acc: 0.7195 - val_loss: 1.3097 - val_acc: 0.7070\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2915 - acc: 0.7215 - val_loss: 1.3047 - val_acc: 0.7080\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2871 - acc: 0.7212 - val_loss: 1.3041 - val_acc: 0.7060\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2829 - acc: 0.7220 - val_loss: 1.3008 - val_acc: 0.7050\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2784 - acc: 0.7229 - val_loss: 1.2912 - val_acc: 0.7080\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2748 - acc: 0.7257 - val_loss: 1.2859 - val_acc: 0.7090\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2706 - acc: 0.7231 - val_loss: 1.2828 - val_acc: 0.7080\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2655 - acc: 0.7237 - val_loss: 1.2760 - val_acc: 0.7090\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2616 - acc: 0.7248 - val_loss: 1.2738 - val_acc: 0.7140\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2580 - acc: 0.7248 - val_loss: 1.2726 - val_acc: 0.7110\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2538 - acc: 0.7239 - val_loss: 1.2681 - val_acc: 0.7120\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2491 - acc: 0.7275 - val_loss: 1.2616 - val_acc: 0.7150\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2458 - acc: 0.7264 - val_loss: 1.2580 - val_acc: 0.7160\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2413 - acc: 0.7277 - val_loss: 1.2547 - val_acc: 0.7110\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2378 - acc: 0.7257 - val_loss: 1.2532 - val_acc: 0.7100\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2338 - acc: 0.7280 - val_loss: 1.2470 - val_acc: 0.7090\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2297 - acc: 0.7277 - val_loss: 1.2434 - val_acc: 0.7120\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2264 - acc: 0.7284 - val_loss: 1.2412 - val_acc: 0.7090\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2229 - acc: 0.7279 - val_loss: 1.2365 - val_acc: 0.7120\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2187 - acc: 0.7299 - val_loss: 1.2299 - val_acc: 0.7160\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2150 - acc: 0.7299 - val_loss: 1.2276 - val_acc: 0.7160\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2119 - acc: 0.7291 - val_loss: 1.2255 - val_acc: 0.7110\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2079 - acc: 0.7315 - val_loss: 1.2205 - val_acc: 0.7160\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2051 - acc: 0.7307 - val_loss: 1.2258 - val_acc: 0.7060\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2015 - acc: 0.7315 - val_loss: 1.2128 - val_acc: 0.7210\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1980 - acc: 0.7309 - val_loss: 1.2075 - val_acc: 0.7200\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1948 - acc: 0.7320 - val_loss: 1.2078 - val_acc: 0.7150\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1914 - acc: 0.7315 - val_loss: 1.2110 - val_acc: 0.7090\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1884 - acc: 0.7321 - val_loss: 1.1993 - val_acc: 0.7200\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1845 - acc: 0.7300 - val_loss: 1.1984 - val_acc: 0.7180\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1812 - acc: 0.7315 - val_loss: 1.1938 - val_acc: 0.7180\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1790 - acc: 0.7336 - val_loss: 1.1997 - val_acc: 0.7160\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1749 - acc: 0.7343 - val_loss: 1.1907 - val_acc: 0.7180\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1723 - acc: 0.7345 - val_loss: 1.1867 - val_acc: 0.7150\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1686 - acc: 0.7347 - val_loss: 1.1873 - val_acc: 0.7120\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1659 - acc: 0.7351 - val_loss: 1.1794 - val_acc: 0.7210\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1629 - acc: 0.7348 - val_loss: 1.1799 - val_acc: 0.7170\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1600 - acc: 0.7352 - val_loss: 1.1749 - val_acc: 0.7230\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1572 - acc: 0.7359 - val_loss: 1.1708 - val_acc: 0.7230\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1545 - acc: 0.7368 - val_loss: 1.1695 - val_acc: 0.7170\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1523 - acc: 0.7357 - val_loss: 1.1666 - val_acc: 0.7170\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1483 - acc: 0.7353 - val_loss: 1.1657 - val_acc: 0.7200\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1460 - acc: 0.7365 - val_loss: 1.1612 - val_acc: 0.7200\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1439 - acc: 0.7365 - val_loss: 1.1570 - val_acc: 0.7190\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1410 - acc: 0.7365 - val_loss: 1.1574 - val_acc: 0.7220\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1380 - acc: 0.7385 - val_loss: 1.1519 - val_acc: 0.7200\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1357 - acc: 0.7371 - val_loss: 1.1495 - val_acc: 0.7230\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1337 - acc: 0.7361 - val_loss: 1.1464 - val_acc: 0.7170\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1301 - acc: 0.7388 - val_loss: 1.1467 - val_acc: 0.7200\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1278 - acc: 0.7361 - val_loss: 1.1467 - val_acc: 0.7220\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1264 - acc: 0.7381 - val_loss: 1.1413 - val_acc: 0.7200\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1233 - acc: 0.7391 - val_loss: 1.1372 - val_acc: 0.7180\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1209 - acc: 0.7388 - val_loss: 1.1363 - val_acc: 0.7240\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1180 - acc: 0.7399 - val_loss: 1.1421 - val_acc: 0.7200\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1167 - acc: 0.7383 - val_loss: 1.1328 - val_acc: 0.7200\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1136 - acc: 0.7395 - val_loss: 1.1292 - val_acc: 0.7220\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1119 - acc: 0.7376 - val_loss: 1.1264 - val_acc: 0.7210\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1089 - acc: 0.7396 - val_loss: 1.1325 - val_acc: 0.7180\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1081 - acc: 0.7417 - val_loss: 1.1217 - val_acc: 0.7250\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1051 - acc: 0.7380 - val_loss: 1.1285 - val_acc: 0.7170\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1032 - acc: 0.7399 - val_loss: 1.1215 - val_acc: 0.7200\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1015 - acc: 0.7389 - val_loss: 1.1390 - val_acc: 0.7160\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0997 - acc: 0.7415 - val_loss: 1.1133 - val_acc: 0.7200\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0971 - acc: 0.7399 - val_loss: 1.1147 - val_acc: 0.7200\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0953 - acc: 0.7423 - val_loss: 1.1093 - val_acc: 0.7240\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0933 - acc: 0.7407 - val_loss: 1.1349 - val_acc: 0.7160\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0922 - acc: 0.7419 - val_loss: 1.1133 - val_acc: 0.7230\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0901 - acc: 0.7413 - val_loss: 1.1098 - val_acc: 0.7230\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0882 - acc: 0.7420 - val_loss: 1.1055 - val_acc: 0.7230\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0865 - acc: 0.7416 - val_loss: 1.1061 - val_acc: 0.7210\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0839 - acc: 0.7416 - val_loss: 1.1024 - val_acc: 0.7240\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0823 - acc: 0.7432 - val_loss: 1.1051 - val_acc: 0.7180\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0806 - acc: 0.7435 - val_loss: 1.0988 - val_acc: 0.7250\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0791 - acc: 0.7427 - val_loss: 1.0953 - val_acc: 0.7210\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0775 - acc: 0.7427 - val_loss: 1.0932 - val_acc: 0.7260\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0753 - acc: 0.7424 - val_loss: 1.1000 - val_acc: 0.7260\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0738 - acc: 0.7417 - val_loss: 1.0895 - val_acc: 0.7210\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0717 - acc: 0.7436 - val_loss: 1.0968 - val_acc: 0.7220\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0710 - acc: 0.7437 - val_loss: 1.0856 - val_acc: 0.7260\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0685 - acc: 0.7463 - val_loss: 1.0852 - val_acc: 0.7210\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0672 - acc: 0.7445 - val_loss: 1.0825 - val_acc: 0.7230\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0651 - acc: 0.7443 - val_loss: 1.0818 - val_acc: 0.7280\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0638 - acc: 0.7463 - val_loss: 1.0836 - val_acc: 0.7260\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0621 - acc: 0.7464 - val_loss: 1.0836 - val_acc: 0.7230\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0614 - acc: 0.7444 - val_loss: 1.0860 - val_acc: 0.7220\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0594 - acc: 0.7467 - val_loss: 1.0779 - val_acc: 0.7290\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0582 - acc: 0.7449 - val_loss: 1.0792 - val_acc: 0.7240\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0561 - acc: 0.7459 - val_loss: 1.0909 - val_acc: 0.7250\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0556 - acc: 0.7440 - val_loss: 1.0737 - val_acc: 0.7320\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0542 - acc: 0.7461 - val_loss: 1.0775 - val_acc: 0.7280\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0517 - acc: 0.7473 - val_loss: 1.0784 - val_acc: 0.7270\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0514 - acc: 0.7457 - val_loss: 1.0745 - val_acc: 0.7290\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0493 - acc: 0.7467 - val_loss: 1.0666 - val_acc: 0.7280\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0476 - acc: 0.7457 - val_loss: 1.0652 - val_acc: 0.7300\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0463 - acc: 0.7477 - val_loss: 1.0668 - val_acc: 0.7280\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0452 - acc: 0.7489 - val_loss: 1.0627 - val_acc: 0.7310\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0428 - acc: 0.7480 - val_loss: 1.0618 - val_acc: 0.7290\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0421 - acc: 0.7481 - val_loss: 1.0632 - val_acc: 0.7240\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0410 - acc: 0.7488 - val_loss: 1.0643 - val_acc: 0.7280\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0390 - acc: 0.7467 - val_loss: 1.0626 - val_acc: 0.7250\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0382 - acc: 0.7489 - val_loss: 1.0608 - val_acc: 0.7260\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0368 - acc: 0.7483 - val_loss: 1.0564 - val_acc: 0.7310\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0351 - acc: 0.7492 - val_loss: 1.0557 - val_acc: 0.7270\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0335 - acc: 0.7480 - val_loss: 1.0576 - val_acc: 0.7270\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0325 - acc: 0.7495 - val_loss: 1.0501 - val_acc: 0.7330\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0313 - acc: 0.7484 - val_loss: 1.0550 - val_acc: 0.7270\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0302 - acc: 0.7488 - val_loss: 1.0504 - val_acc: 0.7310\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0288 - acc: 0.7501 - val_loss: 1.0568 - val_acc: 0.7240\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0278 - acc: 0.7501 - val_loss: 1.0453 - val_acc: 0.7290\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0256 - acc: 0.7513 - val_loss: 1.0517 - val_acc: 0.7260\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0250 - acc: 0.7496 - val_loss: 1.0528 - val_acc: 0.7200\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0232 - acc: 0.7499 - val_loss: 1.0455 - val_acc: 0.7240\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0230 - acc: 0.7511 - val_loss: 1.0410 - val_acc: 0.7270\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0208 - acc: 0.7504 - val_loss: 1.0469 - val_acc: 0.7290\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0197 - acc: 0.7519 - val_loss: 1.0380 - val_acc: 0.7350\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0186 - acc: 0.7533 - val_loss: 1.0400 - val_acc: 0.7280\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0179 - acc: 0.7511 - val_loss: 1.0401 - val_acc: 0.7260\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0158 - acc: 0.7511 - val_loss: 1.0369 - val_acc: 0.7300\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0149 - acc: 0.7513 - val_loss: 1.0384 - val_acc: 0.7250\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0136 - acc: 0.7536 - val_loss: 1.0340 - val_acc: 0.7300\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0125 - acc: 0.7500 - val_loss: 1.0347 - val_acc: 0.7280\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0114 - acc: 0.7529 - val_loss: 1.0357 - val_acc: 0.7310\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0107 - acc: 0.7524 - val_loss: 1.0350 - val_acc: 0.7280\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0090 - acc: 0.7529 - val_loss: 1.0347 - val_acc: 0.7300\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0080 - acc: 0.7519 - val_loss: 1.0422 - val_acc: 0.7230\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0081 - acc: 0.7515 - val_loss: 1.0349 - val_acc: 0.7230\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0060 - acc: 0.7533 - val_loss: 1.0264 - val_acc: 0.7300\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0046 - acc: 0.7529 - val_loss: 1.0278 - val_acc: 0.7250\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0039 - acc: 0.7539 - val_loss: 1.0323 - val_acc: 0.7270\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0024 - acc: 0.7539 - val_loss: 1.0343 - val_acc: 0.7240\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0016 - acc: 0.7537 - val_loss: 1.0292 - val_acc: 0.7250\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0010 - acc: 0.7539 - val_loss: 1.0228 - val_acc: 0.7310\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0001 - acc: 0.7535 - val_loss: 1.0259 - val_acc: 0.7280\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9987 - acc: 0.7529 - val_loss: 1.0252 - val_acc: 0.7270\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9980 - acc: 0.7539 - val_loss: 1.0194 - val_acc: 0.7340\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9968 - acc: 0.7532 - val_loss: 1.0230 - val_acc: 0.7240\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9950 - acc: 0.7548 - val_loss: 1.0199 - val_acc: 0.7330\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9945 - acc: 0.7531 - val_loss: 1.0237 - val_acc: 0.7290\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9934 - acc: 0.7549 - val_loss: 1.0210 - val_acc: 0.7300\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9930 - acc: 0.7560 - val_loss: 1.0199 - val_acc: 0.7260\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9918 - acc: 0.7567 - val_loss: 1.0166 - val_acc: 0.7270\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9910 - acc: 0.7547 - val_loss: 1.0211 - val_acc: 0.7300\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9900 - acc: 0.7548 - val_loss: 1.0147 - val_acc: 0.7240\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9886 - acc: 0.7560 - val_loss: 1.0204 - val_acc: 0.7290\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9885 - acc: 0.7568 - val_loss: 1.0268 - val_acc: 0.7280\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9869 - acc: 0.7559 - val_loss: 1.0121 - val_acc: 0.7360\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9870 - acc: 0.7575 - val_loss: 1.0162 - val_acc: 0.7230\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9851 - acc: 0.7565 - val_loss: 1.0117 - val_acc: 0.7320\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9839 - acc: 0.7595 - val_loss: 1.0098 - val_acc: 0.7280\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9832 - acc: 0.7555 - val_loss: 1.0130 - val_acc: 0.7230\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9824 - acc: 0.7559 - val_loss: 1.0053 - val_acc: 0.7370\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9811 - acc: 0.7564 - val_loss: 1.0196 - val_acc: 0.7230\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9816 - acc: 0.7579 - val_loss: 1.0137 - val_acc: 0.7240\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9793 - acc: 0.7571 - val_loss: 1.0111 - val_acc: 0.7260\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9801 - acc: 0.7568 - val_loss: 1.0138 - val_acc: 0.7220\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9784 - acc: 0.7572 - val_loss: 1.0055 - val_acc: 0.7240\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9775 - acc: 0.7564 - val_loss: 1.0045 - val_acc: 0.7320\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9767 - acc: 0.7580 - val_loss: 1.0045 - val_acc: 0.7300\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9757 - acc: 0.7585 - val_loss: 1.0022 - val_acc: 0.7280\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9753 - acc: 0.7604 - val_loss: 1.0032 - val_acc: 0.7250\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9736 - acc: 0.7597 - val_loss: 1.0192 - val_acc: 0.7210\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9732 - acc: 0.7604 - val_loss: 1.0026 - val_acc: 0.7260\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9723 - acc: 0.7568 - val_loss: 1.0100 - val_acc: 0.7260\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9723 - acc: 0.7584 - val_loss: 1.0006 - val_acc: 0.7250\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9703 - acc: 0.7575 - val_loss: 1.0003 - val_acc: 0.7350\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9709 - acc: 0.7573 - val_loss: 0.9979 - val_acc: 0.7290\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9695 - acc: 0.7587 - val_loss: 0.9991 - val_acc: 0.7300\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9685 - acc: 0.7591 - val_loss: 0.9995 - val_acc: 0.7280\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9681 - acc: 0.7601 - val_loss: 0.9938 - val_acc: 0.7340\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9675 - acc: 0.7616 - val_loss: 0.9989 - val_acc: 0.7260\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9662 - acc: 0.7608 - val_loss: 0.9965 - val_acc: 0.7330\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9663 - acc: 0.7593 - val_loss: 0.9946 - val_acc: 0.7330\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9652 - acc: 0.7603 - val_loss: 1.0043 - val_acc: 0.7310\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9647 - acc: 0.7593 - val_loss: 0.9951 - val_acc: 0.7300\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9642 - acc: 0.7600 - val_loss: 0.9973 - val_acc: 0.7300\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9636 - acc: 0.7592 - val_loss: 0.9927 - val_acc: 0.7280\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9619 - acc: 0.7613 - val_loss: 0.9915 - val_acc: 0.7330\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9614 - acc: 0.7625 - val_loss: 0.9891 - val_acc: 0.7350\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9609 - acc: 0.7613 - val_loss: 0.9884 - val_acc: 0.7420\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9598 - acc: 0.7613 - val_loss: 0.9910 - val_acc: 0.7340\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9600 - acc: 0.7607 - val_loss: 0.9932 - val_acc: 0.7410\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9592 - acc: 0.7616 - val_loss: 1.0009 - val_acc: 0.7280\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9588 - acc: 0.7597 - val_loss: 0.9950 - val_acc: 0.7260\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9581 - acc: 0.7625 - val_loss: 0.9976 - val_acc: 0.7290\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9570 - acc: 0.7607 - val_loss: 0.9854 - val_acc: 0.7290\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9563 - acc: 0.7608 - val_loss: 0.9912 - val_acc: 0.7230\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9555 - acc: 0.7624 - val_loss: 0.9923 - val_acc: 0.7350\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9555 - acc: 0.7629 - val_loss: 0.9850 - val_acc: 0.7360\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9547 - acc: 0.7617 - val_loss: 0.9889 - val_acc: 0.7330\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9536 - acc: 0.7617 - val_loss: 0.9833 - val_acc: 0.7350\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9532 - acc: 0.7631 - val_loss: 0.9834 - val_acc: 0.7370\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9525 - acc: 0.7648 - val_loss: 0.9878 - val_acc: 0.7370\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9524 - acc: 0.7616 - val_loss: 0.9821 - val_acc: 0.7390\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9517 - acc: 0.7656 - val_loss: 0.9799 - val_acc: 0.7420\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9508 - acc: 0.7639 - val_loss: 0.9870 - val_acc: 0.7290\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9501 - acc: 0.7628 - val_loss: 0.9840 - val_acc: 0.7370\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9503 - acc: 0.7620 - val_loss: 0.9811 - val_acc: 0.7320\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9490 - acc: 0.7640 - val_loss: 0.9810 - val_acc: 0.7380\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9482 - acc: 0.7620 - val_loss: 0.9966 - val_acc: 0.7310\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9483 - acc: 0.7632 - val_loss: 0.9793 - val_acc: 0.7370\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9474 - acc: 0.7667 - val_loss: 0.9797 - val_acc: 0.7360\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9464 - acc: 0.7636 - val_loss: 0.9847 - val_acc: 0.7300\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9472 - acc: 0.7624 - val_loss: 0.9788 - val_acc: 0.7300\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9458 - acc: 0.7644 - val_loss: 0.9798 - val_acc: 0.7300\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9448 - acc: 0.7648 - val_loss: 0.9789 - val_acc: 0.7370\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9448 - acc: 0.7624 - val_loss: 0.9803 - val_acc: 0.7380\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9443 - acc: 0.7644 - val_loss: 0.9882 - val_acc: 0.7330\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9443 - acc: 0.7632 - val_loss: 0.9773 - val_acc: 0.7370\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9431 - acc: 0.7655 - val_loss: 0.9765 - val_acc: 0.7390\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9422 - acc: 0.7661 - val_loss: 0.9793 - val_acc: 0.7370\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9423 - acc: 0.7652 - val_loss: 0.9809 - val_acc: 0.7310\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9416 - acc: 0.7652 - val_loss: 0.9720 - val_acc: 0.7380\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9410 - acc: 0.7645 - val_loss: 0.9735 - val_acc: 0.7420\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9402 - acc: 0.7636 - val_loss: 0.9780 - val_acc: 0.7360\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9400 - acc: 0.7641 - val_loss: 0.9700 - val_acc: 0.7420\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9394 - acc: 0.7657 - val_loss: 0.9779 - val_acc: 0.7320\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9395 - acc: 0.7652 - val_loss: 0.9838 - val_acc: 0.7350\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9386 - acc: 0.7656 - val_loss: 0.9723 - val_acc: 0.7350\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9381 - acc: 0.7645 - val_loss: 0.9715 - val_acc: 0.7370\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9375 - acc: 0.7649 - val_loss: 0.9704 - val_acc: 0.7390\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9374 - acc: 0.7639 - val_loss: 0.9742 - val_acc: 0.7410\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9369 - acc: 0.7664 - val_loss: 0.9777 - val_acc: 0.7380\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9371 - acc: 0.7673 - val_loss: 0.9720 - val_acc: 0.7360\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9355 - acc: 0.7676 - val_loss: 0.9737 - val_acc: 0.7380\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9358 - acc: 0.7677 - val_loss: 0.9798 - val_acc: 0.7360\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9350 - acc: 0.7665 - val_loss: 0.9687 - val_acc: 0.7410\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9347 - acc: 0.7688 - val_loss: 0.9659 - val_acc: 0.7430\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9331 - acc: 0.7673 - val_loss: 0.9720 - val_acc: 0.7420\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9333 - acc: 0.7664 - val_loss: 0.9713 - val_acc: 0.7410\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9330 - acc: 0.7669 - val_loss: 0.9697 - val_acc: 0.7400\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9318 - acc: 0.7692 - val_loss: 0.9676 - val_acc: 0.7400\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9324 - acc: 0.7667 - val_loss: 0.9689 - val_acc: 0.7380\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9323 - acc: 0.7672 - val_loss: 0.9876 - val_acc: 0.7310\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9320 - acc: 0.7655 - val_loss: 0.9668 - val_acc: 0.7430\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9316 - acc: 0.7681 - val_loss: 0.9681 - val_acc: 0.7410\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9306 - acc: 0.7665 - val_loss: 0.9693 - val_acc: 0.7400\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9294 - acc: 0.7691 - val_loss: 0.9655 - val_acc: 0.7420\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9290 - acc: 0.7692 - val_loss: 0.9712 - val_acc: 0.7360\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9294 - acc: 0.7657 - val_loss: 0.9730 - val_acc: 0.7400\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9290 - acc: 0.7700 - val_loss: 0.9680 - val_acc: 0.7370\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9275 - acc: 0.7715 - val_loss: 0.9659 - val_acc: 0.7430\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9275 - acc: 0.7684 - val_loss: 0.9635 - val_acc: 0.7400\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9263 - acc: 0.7675 - val_loss: 0.9651 - val_acc: 0.7380\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9259 - acc: 0.7691 - val_loss: 0.9614 - val_acc: 0.7420\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9255 - acc: 0.7683 - val_loss: 0.9647 - val_acc: 0.7410\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9254 - acc: 0.7685 - val_loss: 0.9693 - val_acc: 0.7410\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9249 - acc: 0.7675 - val_loss: 0.9597 - val_acc: 0.7450\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9253 - acc: 0.7689 - val_loss: 0.9658 - val_acc: 0.7390\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9247 - acc: 0.7683 - val_loss: 0.9616 - val_acc: 0.7440\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9242 - acc: 0.7680 - val_loss: 0.9588 - val_acc: 0.7400\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9231 - acc: 0.7685 - val_loss: 0.9683 - val_acc: 0.7370\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9238 - acc: 0.7681 - val_loss: 0.9676 - val_acc: 0.7360\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9230 - acc: 0.7696 - val_loss: 0.9645 - val_acc: 0.7410\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9223 - acc: 0.7687 - val_loss: 0.9633 - val_acc: 0.7410\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9212 - acc: 0.7701 - val_loss: 0.9585 - val_acc: 0.7410\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9216 - acc: 0.7693 - val_loss: 0.9691 - val_acc: 0.7450\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9212 - acc: 0.7720 - val_loss: 0.9701 - val_acc: 0.7360\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9204 - acc: 0.7695 - val_loss: 0.9572 - val_acc: 0.7490\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9201 - acc: 0.7711 - val_loss: 0.9592 - val_acc: 0.7390\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9195 - acc: 0.7684 - val_loss: 0.9595 - val_acc: 0.7470\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9194 - acc: 0.7685 - val_loss: 0.9561 - val_acc: 0.7420\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9186 - acc: 0.7705 - val_loss: 0.9748 - val_acc: 0.7330\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9189 - acc: 0.7711 - val_loss: 0.9562 - val_acc: 0.7470\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9179 - acc: 0.7711 - val_loss: 0.9641 - val_acc: 0.7410\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9179 - acc: 0.7693 - val_loss: 0.9663 - val_acc: 0.7340\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9176 - acc: 0.7687 - val_loss: 0.9533 - val_acc: 0.7440\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9168 - acc: 0.7713 - val_loss: 0.9560 - val_acc: 0.7420\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9165 - acc: 0.7717 - val_loss: 0.9529 - val_acc: 0.7450\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9161 - acc: 0.7695 - val_loss: 0.9564 - val_acc: 0.7450\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9161 - acc: 0.7708 - val_loss: 0.9527 - val_acc: 0.7450\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9149 - acc: 0.7707 - val_loss: 0.9557 - val_acc: 0.7440\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9149 - acc: 0.7713 - val_loss: 0.9586 - val_acc: 0.7460\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9137 - acc: 0.7713 - val_loss: 0.9539 - val_acc: 0.7440\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9139 - acc: 0.7712 - val_loss: 0.9567 - val_acc: 0.7490\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9138 - acc: 0.7720 - val_loss: 0.9518 - val_acc: 0.7460\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9134 - acc: 0.7707 - val_loss: 0.9543 - val_acc: 0.7490\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9131 - acc: 0.7712 - val_loss: 0.9510 - val_acc: 0.7480\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9125 - acc: 0.7705 - val_loss: 0.9535 - val_acc: 0.7480\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9122 - acc: 0.7705 - val_loss: 0.9548 - val_acc: 0.7450\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9123 - acc: 0.7719 - val_loss: 0.9627 - val_acc: 0.7350\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9111 - acc: 0.7716 - val_loss: 0.9553 - val_acc: 0.7470\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9116 - acc: 0.7708 - val_loss: 0.9500 - val_acc: 0.7520\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9110 - acc: 0.7701 - val_loss: 0.9496 - val_acc: 0.7460\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9107 - acc: 0.7713 - val_loss: 0.9486 - val_acc: 0.7490\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9108 - acc: 0.7703 - val_loss: 0.9519 - val_acc: 0.7480\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9093 - acc: 0.7721 - val_loss: 0.9536 - val_acc: 0.7450\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9096 - acc: 0.7720 - val_loss: 0.9590 - val_acc: 0.7430\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9095 - acc: 0.7707 - val_loss: 0.9486 - val_acc: 0.7510\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9082 - acc: 0.7721 - val_loss: 0.9580 - val_acc: 0.7380\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9080 - acc: 0.7720 - val_loss: 0.9586 - val_acc: 0.7390\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9076 - acc: 0.7720 - val_loss: 0.9540 - val_acc: 0.7400\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9088 - acc: 0.7721 - val_loss: 0.9519 - val_acc: 0.7460\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9077 - acc: 0.7699 - val_loss: 0.9618 - val_acc: 0.7450\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9067 - acc: 0.7720 - val_loss: 0.9520 - val_acc: 0.7470\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9066 - acc: 0.7731 - val_loss: 0.9656 - val_acc: 0.7320\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9059 - acc: 0.7735 - val_loss: 0.9551 - val_acc: 0.7380\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9062 - acc: 0.7723 - val_loss: 0.9639 - val_acc: 0.7270\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9058 - acc: 0.7725 - val_loss: 0.9467 - val_acc: 0.7530\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9055 - acc: 0.7731 - val_loss: 0.9503 - val_acc: 0.7500\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9047 - acc: 0.7737 - val_loss: 0.9645 - val_acc: 0.7370\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9051 - acc: 0.7721 - val_loss: 0.9461 - val_acc: 0.7510\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9037 - acc: 0.7729 - val_loss: 0.9557 - val_acc: 0.7420\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9040 - acc: 0.7739 - val_loss: 0.9474 - val_acc: 0.7530\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9034 - acc: 0.7737 - val_loss: 0.9517 - val_acc: 0.7390\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9035 - acc: 0.7728 - val_loss: 0.9627 - val_acc: 0.7490\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9026 - acc: 0.7737 - val_loss: 0.9679 - val_acc: 0.7460\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9038 - acc: 0.7723 - val_loss: 0.9470 - val_acc: 0.7490\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9020 - acc: 0.7729 - val_loss: 0.9533 - val_acc: 0.7440\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9014 - acc: 0.7745 - val_loss: 0.9477 - val_acc: 0.7440\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9009 - acc: 0.7729 - val_loss: 0.9649 - val_acc: 0.7470\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9019 - acc: 0.7739 - val_loss: 0.9530 - val_acc: 0.7470\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9009 - acc: 0.7724 - val_loss: 0.9451 - val_acc: 0.7510\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9008 - acc: 0.7723 - val_loss: 0.9474 - val_acc: 0.7460\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9009 - acc: 0.7719 - val_loss: 0.9458 - val_acc: 0.7470\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8987 - acc: 0.7737 - val_loss: 0.9698 - val_acc: 0.7270\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8999 - acc: 0.7717 - val_loss: 0.9615 - val_acc: 0.7390\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8991 - acc: 0.7727 - val_loss: 0.9512 - val_acc: 0.7450\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8993 - acc: 0.7731 - val_loss: 0.9433 - val_acc: 0.7520\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8980 - acc: 0.7739 - val_loss: 0.9409 - val_acc: 0.7510\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8979 - acc: 0.7745 - val_loss: 0.9416 - val_acc: 0.7470\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8985 - acc: 0.7723 - val_loss: 0.9450 - val_acc: 0.7460\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8972 - acc: 0.7731 - val_loss: 0.9453 - val_acc: 0.7500\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8972 - acc: 0.7740 - val_loss: 0.9411 - val_acc: 0.7520\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8966 - acc: 0.7733 - val_loss: 0.9427 - val_acc: 0.7480\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8972 - acc: 0.7735 - val_loss: 0.9440 - val_acc: 0.7470\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8965 - acc: 0.7748 - val_loss: 0.9421 - val_acc: 0.7510\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8964 - acc: 0.7755 - val_loss: 0.9502 - val_acc: 0.7500\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8960 - acc: 0.7740 - val_loss: 0.9412 - val_acc: 0.7520\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8947 - acc: 0.7741 - val_loss: 0.9435 - val_acc: 0.7440\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8943 - acc: 0.7745 - val_loss: 0.9396 - val_acc: 0.7530\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8945 - acc: 0.7728 - val_loss: 0.9437 - val_acc: 0.7460\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8946 - acc: 0.7733 - val_loss: 0.9488 - val_acc: 0.7480\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8950 - acc: 0.7731 - val_loss: 0.9532 - val_acc: 0.7410\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8933 - acc: 0.7755 - val_loss: 0.9381 - val_acc: 0.7480\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8937 - acc: 0.7751 - val_loss: 0.9373 - val_acc: 0.7560\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8923 - acc: 0.7747 - val_loss: 0.9487 - val_acc: 0.7450\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8924 - acc: 0.7755 - val_loss: 0.9474 - val_acc: 0.7400\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8930 - acc: 0.7735 - val_loss: 0.9471 - val_acc: 0.7490\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8928 - acc: 0.7761 - val_loss: 0.9503 - val_acc: 0.7350\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8922 - acc: 0.7739 - val_loss: 0.9359 - val_acc: 0.7530\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8910 - acc: 0.7768 - val_loss: 0.9507 - val_acc: 0.7400\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8913 - acc: 0.7748 - val_loss: 0.9419 - val_acc: 0.7530\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8909 - acc: 0.7736 - val_loss: 0.9465 - val_acc: 0.7490\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8909 - acc: 0.7760 - val_loss: 0.9422 - val_acc: 0.7500\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8913 - acc: 0.7743 - val_loss: 0.9388 - val_acc: 0.7540\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8907 - acc: 0.7741 - val_loss: 0.9388 - val_acc: 0.7520\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8897 - acc: 0.7751 - val_loss: 0.9384 - val_acc: 0.7570\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8899 - acc: 0.7751 - val_loss: 0.9444 - val_acc: 0.7460\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8902 - acc: 0.7735 - val_loss: 0.9405 - val_acc: 0.7490\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8893 - acc: 0.7752 - val_loss: 0.9446 - val_acc: 0.7490\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8892 - acc: 0.7733 - val_loss: 0.9418 - val_acc: 0.7440\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8885 - acc: 0.7767 - val_loss: 0.9400 - val_acc: 0.7480\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8885 - acc: 0.7753 - val_loss: 0.9406 - val_acc: 0.7500\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8882 - acc: 0.7745 - val_loss: 0.9363 - val_acc: 0.7530\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8875 - acc: 0.7752 - val_loss: 0.9342 - val_acc: 0.7530\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8881 - acc: 0.7749 - val_loss: 0.9467 - val_acc: 0.7530\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8876 - acc: 0.7748 - val_loss: 0.9376 - val_acc: 0.7540\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8865 - acc: 0.7761 - val_loss: 0.9441 - val_acc: 0.7430\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8872 - acc: 0.7749 - val_loss: 0.9476 - val_acc: 0.7400\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8864 - acc: 0.7756 - val_loss: 0.9370 - val_acc: 0.7480\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8854 - acc: 0.7765 - val_loss: 0.9353 - val_acc: 0.7500\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8856 - acc: 0.7755 - val_loss: 0.9369 - val_acc: 0.7500\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8850 - acc: 0.7756 - val_loss: 0.9534 - val_acc: 0.7480\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8865 - acc: 0.7779 - val_loss: 0.9396 - val_acc: 0.7480\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8846 - acc: 0.7761 - val_loss: 0.9350 - val_acc: 0.7520\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8846 - acc: 0.7772 - val_loss: 0.9424 - val_acc: 0.7450\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8836 - acc: 0.7765 - val_loss: 0.9391 - val_acc: 0.7470\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8837 - acc: 0.7776 - val_loss: 0.9368 - val_acc: 0.7550\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8830 - acc: 0.7777 - val_loss: 0.9335 - val_acc: 0.7510\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8827 - acc: 0.7781 - val_loss: 0.9430 - val_acc: 0.7480\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8823 - acc: 0.7755 - val_loss: 0.9407 - val_acc: 0.7510\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8834 - acc: 0.7757 - val_loss: 0.9492 - val_acc: 0.7480\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8839 - acc: 0.7760 - val_loss: 0.9404 - val_acc: 0.7520\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8821 - acc: 0.7785 - val_loss: 0.9323 - val_acc: 0.7510\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8826 - acc: 0.7781 - val_loss: 0.9339 - val_acc: 0.7530\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8824 - acc: 0.7759 - val_loss: 0.9339 - val_acc: 0.7490\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8814 - acc: 0.7751 - val_loss: 0.9340 - val_acc: 0.7540\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8813 - acc: 0.7767 - val_loss: 0.9424 - val_acc: 0.7500\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8818 - acc: 0.7772 - val_loss: 0.9377 - val_acc: 0.7490\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8806 - acc: 0.7761 - val_loss: 0.9356 - val_acc: 0.7540\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8812 - acc: 0.7783 - val_loss: 0.9335 - val_acc: 0.7510\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8797 - acc: 0.7797 - val_loss: 0.9384 - val_acc: 0.7450\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8810 - acc: 0.7767 - val_loss: 0.9489 - val_acc: 0.7460\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8797 - acc: 0.7799 - val_loss: 0.9333 - val_acc: 0.7520\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8807 - acc: 0.7779 - val_loss: 0.9531 - val_acc: 0.7370\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8800 - acc: 0.7767 - val_loss: 0.9310 - val_acc: 0.7500\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8803 - acc: 0.7796 - val_loss: 0.9388 - val_acc: 0.7490\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8787 - acc: 0.7787 - val_loss: 0.9386 - val_acc: 0.7520\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8796 - acc: 0.7775 - val_loss: 0.9350 - val_acc: 0.7500\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8784 - acc: 0.7772 - val_loss: 0.9361 - val_acc: 0.7460\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8795 - acc: 0.7800 - val_loss: 0.9482 - val_acc: 0.7450\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8789 - acc: 0.7771 - val_loss: 0.9324 - val_acc: 0.7560\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8798 - acc: 0.7773 - val_loss: 0.9359 - val_acc: 0.7540\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8786 - acc: 0.7776 - val_loss: 0.9469 - val_acc: 0.7340\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8789 - acc: 0.7781 - val_loss: 0.9443 - val_acc: 0.7370\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8768 - acc: 0.7792 - val_loss: 0.9428 - val_acc: 0.7440\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8787 - acc: 0.7777 - val_loss: 0.9416 - val_acc: 0.7370\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8777 - acc: 0.7800 - val_loss: 0.9341 - val_acc: 0.7490\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8774 - acc: 0.7781 - val_loss: 0.9387 - val_acc: 0.7430\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8776 - acc: 0.7783 - val_loss: 0.9301 - val_acc: 0.7530\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8761 - acc: 0.7784 - val_loss: 0.9401 - val_acc: 0.7460\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8767 - acc: 0.7785 - val_loss: 0.9344 - val_acc: 0.7510\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8754 - acc: 0.7808 - val_loss: 0.9387 - val_acc: 0.7500\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8777 - acc: 0.7776 - val_loss: 0.9332 - val_acc: 0.7500\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8756 - acc: 0.7781 - val_loss: 0.9281 - val_acc: 0.7500\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8748 - acc: 0.7808 - val_loss: 0.9269 - val_acc: 0.7530\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8749 - acc: 0.7787 - val_loss: 0.9383 - val_acc: 0.7460\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8756 - acc: 0.7792 - val_loss: 0.9279 - val_acc: 0.7540\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8746 - acc: 0.7785 - val_loss: 0.9284 - val_acc: 0.7510\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8742 - acc: 0.7796 - val_loss: 0.9416 - val_acc: 0.7500\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8741 - acc: 0.7791 - val_loss: 0.9291 - val_acc: 0.7450\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8729 - acc: 0.7788 - val_loss: 0.9364 - val_acc: 0.7470\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8731 - acc: 0.7789 - val_loss: 0.9284 - val_acc: 0.7520\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8735 - acc: 0.7813 - val_loss: 0.9319 - val_acc: 0.7470\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8746 - acc: 0.7793 - val_loss: 0.9383 - val_acc: 0.7480\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8738 - acc: 0.7817 - val_loss: 0.9565 - val_acc: 0.7310\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8734 - acc: 0.7785 - val_loss: 0.9406 - val_acc: 0.7410\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8729 - acc: 0.7807 - val_loss: 0.9349 - val_acc: 0.7450\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8714 - acc: 0.7805 - val_loss: 0.9304 - val_acc: 0.7500\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8716 - acc: 0.7803 - val_loss: 0.9338 - val_acc: 0.7480\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8715 - acc: 0.7793 - val_loss: 0.9320 - val_acc: 0.7510\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8719 - acc: 0.7793 - val_loss: 0.9322 - val_acc: 0.7480\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8710 - acc: 0.7804 - val_loss: 0.9345 - val_acc: 0.7480\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8700 - acc: 0.7836 - val_loss: 0.9286 - val_acc: 0.7490\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8716 - acc: 0.7791 - val_loss: 0.9270 - val_acc: 0.7570\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8707 - acc: 0.7803 - val_loss: 0.9285 - val_acc: 0.7520\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8709 - acc: 0.7796 - val_loss: 0.9432 - val_acc: 0.7480\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8697 - acc: 0.7792 - val_loss: 0.9320 - val_acc: 0.7430\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8706 - acc: 0.7776 - val_loss: 0.9420 - val_acc: 0.7380\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8707 - acc: 0.7820 - val_loss: 0.9256 - val_acc: 0.7520\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8698 - acc: 0.7812 - val_loss: 0.9337 - val_acc: 0.7450\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8698 - acc: 0.7817 - val_loss: 0.9383 - val_acc: 0.7490\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8696 - acc: 0.7801 - val_loss: 0.9421 - val_acc: 0.7450\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8705 - acc: 0.7796 - val_loss: 0.9436 - val_acc: 0.7450\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8700 - acc: 0.7803 - val_loss: 0.9481 - val_acc: 0.7500\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8690 - acc: 0.7817 - val_loss: 0.9276 - val_acc: 0.7510\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8682 - acc: 0.7820 - val_loss: 0.9489 - val_acc: 0.7280\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8694 - acc: 0.7803 - val_loss: 0.9277 - val_acc: 0.7500\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8680 - acc: 0.7807 - val_loss: 0.9367 - val_acc: 0.7500\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8690 - acc: 0.7793 - val_loss: 0.9300 - val_acc: 0.7490\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8675 - acc: 0.7823 - val_loss: 0.9349 - val_acc: 0.7520\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8685 - acc: 0.7801 - val_loss: 0.9281 - val_acc: 0.7500\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8670 - acc: 0.7795 - val_loss: 0.9254 - val_acc: 0.7530\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8674 - acc: 0.7832 - val_loss: 0.9376 - val_acc: 0.7540\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8676 - acc: 0.7816 - val_loss: 0.9464 - val_acc: 0.7490\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8662 - acc: 0.7823 - val_loss: 0.9373 - val_acc: 0.7400\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8670 - acc: 0.7831 - val_loss: 0.9251 - val_acc: 0.7530\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8656 - acc: 0.7855 - val_loss: 0.9249 - val_acc: 0.7530\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8663 - acc: 0.7823 - val_loss: 0.9357 - val_acc: 0.7420\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8657 - acc: 0.7848 - val_loss: 0.9314 - val_acc: 0.7480\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8657 - acc: 0.7824 - val_loss: 0.9280 - val_acc: 0.7550\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8656 - acc: 0.7824 - val_loss: 0.9263 - val_acc: 0.7490\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8652 - acc: 0.7827 - val_loss: 0.9333 - val_acc: 0.7500\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8662 - acc: 0.7781 - val_loss: 0.9262 - val_acc: 0.7540\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8646 - acc: 0.7825 - val_loss: 0.9413 - val_acc: 0.7430\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8651 - acc: 0.7832 - val_loss: 0.9390 - val_acc: 0.7460\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8650 - acc: 0.7813 - val_loss: 0.9261 - val_acc: 0.7470\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8638 - acc: 0.7815 - val_loss: 0.9270 - val_acc: 0.7490\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8629 - acc: 0.7829 - val_loss: 0.9236 - val_acc: 0.7530\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8642 - acc: 0.7817 - val_loss: 0.9359 - val_acc: 0.7500\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8649 - acc: 0.7828 - val_loss: 0.9279 - val_acc: 0.7510\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8627 - acc: 0.7831 - val_loss: 0.9301 - val_acc: 0.7490\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8625 - acc: 0.7833 - val_loss: 0.9247 - val_acc: 0.7460\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8627 - acc: 0.7821 - val_loss: 0.9229 - val_acc: 0.7530\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8632 - acc: 0.7827 - val_loss: 0.9622 - val_acc: 0.7440\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8629 - acc: 0.7828 - val_loss: 0.9304 - val_acc: 0.7470\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8642 - acc: 0.7825 - val_loss: 0.9330 - val_acc: 0.7480\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8624 - acc: 0.7797 - val_loss: 0.9275 - val_acc: 0.7580\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8632 - acc: 0.7836 - val_loss: 0.9255 - val_acc: 0.7480\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8615 - acc: 0.7827 - val_loss: 0.9707 - val_acc: 0.7290\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8629 - acc: 0.7805 - val_loss: 0.9248 - val_acc: 0.7500\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8619 - acc: 0.7837 - val_loss: 0.9242 - val_acc: 0.7530\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8606 - acc: 0.7823 - val_loss: 0.9262 - val_acc: 0.7500\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8611 - acc: 0.7825 - val_loss: 0.9237 - val_acc: 0.7490\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8612 - acc: 0.7832 - val_loss: 0.9245 - val_acc: 0.7540\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8613 - acc: 0.7835 - val_loss: 0.9283 - val_acc: 0.7470\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8606 - acc: 0.7815 - val_loss: 0.9301 - val_acc: 0.7500\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8600 - acc: 0.7828 - val_loss: 0.9494 - val_acc: 0.7490\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8605 - acc: 0.7811 - val_loss: 0.9364 - val_acc: 0.7470\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8605 - acc: 0.7821 - val_loss: 0.9428 - val_acc: 0.7350\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8596 - acc: 0.7841 - val_loss: 0.9246 - val_acc: 0.7520\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8587 - acc: 0.7824 - val_loss: 0.9204 - val_acc: 0.7540\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8590 - acc: 0.7821 - val_loss: 0.9465 - val_acc: 0.7510\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8599 - acc: 0.7840 - val_loss: 0.9278 - val_acc: 0.7490\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8594 - acc: 0.7843 - val_loss: 0.9203 - val_acc: 0.7480\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8584 - acc: 0.7841 - val_loss: 0.9348 - val_acc: 0.7480\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8598 - acc: 0.7836 - val_loss: 0.9250 - val_acc: 0.7490\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8593 - acc: 0.7839 - val_loss: 0.9278 - val_acc: 0.7440\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8575 - acc: 0.7825 - val_loss: 0.9194 - val_acc: 0.7580\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8579 - acc: 0.7828 - val_loss: 0.9219 - val_acc: 0.7540\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8574 - acc: 0.7845 - val_loss: 0.9373 - val_acc: 0.7450\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8594 - acc: 0.7820 - val_loss: 0.9230 - val_acc: 0.7470\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8581 - acc: 0.7832 - val_loss: 0.9344 - val_acc: 0.7460\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8593 - acc: 0.7815 - val_loss: 0.9359 - val_acc: 0.7390\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8573 - acc: 0.7863 - val_loss: 0.9268 - val_acc: 0.7450\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8563 - acc: 0.7856 - val_loss: 0.9256 - val_acc: 0.7560\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8572 - acc: 0.7835 - val_loss: 0.9446 - val_acc: 0.7340\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8563 - acc: 0.7828 - val_loss: 0.9218 - val_acc: 0.7500\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8567 - acc: 0.7823 - val_loss: 0.9188 - val_acc: 0.7540\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8566 - acc: 0.7852 - val_loss: 0.9263 - val_acc: 0.7440\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8571 - acc: 0.7852 - val_loss: 0.9353 - val_acc: 0.7500\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8552 - acc: 0.7853 - val_loss: 0.9242 - val_acc: 0.7530\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8555 - acc: 0.7851 - val_loss: 0.9185 - val_acc: 0.7480\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8560 - acc: 0.7852 - val_loss: 0.9197 - val_acc: 0.7440\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8549 - acc: 0.7857 - val_loss: 0.9263 - val_acc: 0.7430\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8544 - acc: 0.7845 - val_loss: 0.9200 - val_acc: 0.7530\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8554 - acc: 0.7847 - val_loss: 0.9205 - val_acc: 0.7480\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8528 - acc: 0.7864 - val_loss: 0.9531 - val_acc: 0.7390\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8558 - acc: 0.7849 - val_loss: 0.9206 - val_acc: 0.7530\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8532 - acc: 0.7856 - val_loss: 0.9318 - val_acc: 0.7430\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8535 - acc: 0.7839 - val_loss: 0.9216 - val_acc: 0.7490\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8525 - acc: 0.7864 - val_loss: 0.9169 - val_acc: 0.7540\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8520 - acc: 0.7865 - val_loss: 0.9361 - val_acc: 0.7430\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8528 - acc: 0.7855 - val_loss: 0.9432 - val_acc: 0.7480\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8528 - acc: 0.7841 - val_loss: 0.9253 - val_acc: 0.7560\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8526 - acc: 0.7835 - val_loss: 0.9206 - val_acc: 0.7510\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8544 - acc: 0.7832 - val_loss: 0.9183 - val_acc: 0.7500\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8506 - acc: 0.7837 - val_loss: 0.9202 - val_acc: 0.7560\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8515 - acc: 0.7876 - val_loss: 0.9202 - val_acc: 0.7500\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8518 - acc: 0.7845 - val_loss: 0.9193 - val_acc: 0.7480\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8523 - acc: 0.7852 - val_loss: 0.9181 - val_acc: 0.7520\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8505 - acc: 0.7864 - val_loss: 0.9201 - val_acc: 0.7500\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8508 - acc: 0.7877 - val_loss: 0.9222 - val_acc: 0.7500\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8512 - acc: 0.7836 - val_loss: 0.9176 - val_acc: 0.7510\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8521 - acc: 0.7857 - val_loss: 0.9182 - val_acc: 0.7530\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8495 - acc: 0.7876 - val_loss: 0.9201 - val_acc: 0.7470\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8503 - acc: 0.7863 - val_loss: 0.9153 - val_acc: 0.7560\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8488 - acc: 0.7848 - val_loss: 0.9225 - val_acc: 0.7480\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8486 - acc: 0.7871 - val_loss: 0.9298 - val_acc: 0.7390\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8512 - acc: 0.7869 - val_loss: 0.9466 - val_acc: 0.7300\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8497 - acc: 0.7867 - val_loss: 0.9213 - val_acc: 0.7530\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8486 - acc: 0.7867 - val_loss: 0.9332 - val_acc: 0.7440\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8502 - acc: 0.7868 - val_loss: 0.9501 - val_acc: 0.7390\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8500 - acc: 0.7888 - val_loss: 0.9233 - val_acc: 0.7580\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8487 - acc: 0.7864 - val_loss: 0.9250 - val_acc: 0.7420\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8486 - acc: 0.7855 - val_loss: 0.9228 - val_acc: 0.7450\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8476 - acc: 0.7855 - val_loss: 0.9230 - val_acc: 0.7460\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8475 - acc: 0.7839 - val_loss: 0.9196 - val_acc: 0.7480\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8489 - acc: 0.7875 - val_loss: 0.9191 - val_acc: 0.7460\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8462 - acc: 0.7876 - val_loss: 0.9220 - val_acc: 0.7480\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8482 - acc: 0.7857 - val_loss: 0.9290 - val_acc: 0.7410\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8491 - acc: 0.7869 - val_loss: 0.9149 - val_acc: 0.7530\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8487 - acc: 0.7859 - val_loss: 0.9444 - val_acc: 0.7440\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8474 - acc: 0.7875 - val_loss: 0.9182 - val_acc: 0.7530\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8465 - acc: 0.7881 - val_loss: 0.9290 - val_acc: 0.7450\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8472 - acc: 0.7859 - val_loss: 0.9219 - val_acc: 0.7480\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8471 - acc: 0.7876 - val_loss: 0.9217 - val_acc: 0.7580\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8452 - acc: 0.7863 - val_loss: 0.9157 - val_acc: 0.7510\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8460 - acc: 0.7859 - val_loss: 0.9348 - val_acc: 0.7420\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8456 - acc: 0.7889 - val_loss: 0.9450 - val_acc: 0.7490\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8465 - acc: 0.7859 - val_loss: 0.9160 - val_acc: 0.7520\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8458 - acc: 0.7867 - val_loss: 0.9240 - val_acc: 0.7390\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8456 - acc: 0.7879 - val_loss: 0.9172 - val_acc: 0.7480\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8457 - acc: 0.7867 - val_loss: 0.9130 - val_acc: 0.7530\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8445 - acc: 0.7877 - val_loss: 0.9258 - val_acc: 0.7490\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8442 - acc: 0.7888 - val_loss: 0.9204 - val_acc: 0.7520\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8444 - acc: 0.7892 - val_loss: 0.9222 - val_acc: 0.7520\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8435 - acc: 0.7865 - val_loss: 0.9119 - val_acc: 0.7490\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8434 - acc: 0.7881 - val_loss: 0.9431 - val_acc: 0.7510\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8450 - acc: 0.7859 - val_loss: 0.9165 - val_acc: 0.7520\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8436 - acc: 0.7879 - val_loss: 0.9218 - val_acc: 0.7450\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8420 - acc: 0.7873 - val_loss: 0.9259 - val_acc: 0.7440\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8453 - acc: 0.7847 - val_loss: 0.9156 - val_acc: 0.7510\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8411 - acc: 0.7883 - val_loss: 0.9206 - val_acc: 0.7510\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8418 - acc: 0.7901 - val_loss: 0.9145 - val_acc: 0.7520\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8425 - acc: 0.7901 - val_loss: 0.9108 - val_acc: 0.7510\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8401 - acc: 0.7897 - val_loss: 0.9136 - val_acc: 0.7510\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8419 - acc: 0.7884 - val_loss: 0.9222 - val_acc: 0.7460\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8419 - acc: 0.7887 - val_loss: 0.9151 - val_acc: 0.7520\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8414 - acc: 0.7892 - val_loss: 0.9190 - val_acc: 0.7470\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8439 - acc: 0.7873 - val_loss: 0.9145 - val_acc: 0.7510\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8411 - acc: 0.7889 - val_loss: 0.9156 - val_acc: 0.7500\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8430 - acc: 0.7861 - val_loss: 0.9346 - val_acc: 0.7450\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8426 - acc: 0.7888 - val_loss: 0.9370 - val_acc: 0.7450\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8413 - acc: 0.7911 - val_loss: 0.9272 - val_acc: 0.7420\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8407 - acc: 0.7896 - val_loss: 0.9231 - val_acc: 0.7490\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8410 - acc: 0.7895 - val_loss: 0.9209 - val_acc: 0.7480\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8395 - acc: 0.7901 - val_loss: 0.9133 - val_acc: 0.7500\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8396 - acc: 0.7905 - val_loss: 0.9287 - val_acc: 0.7420\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8399 - acc: 0.7900 - val_loss: 0.9112 - val_acc: 0.7560\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8389 - acc: 0.7887 - val_loss: 0.9244 - val_acc: 0.7500\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8402 - acc: 0.7913 - val_loss: 0.9225 - val_acc: 0.7460\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8401 - acc: 0.7924 - val_loss: 0.9164 - val_acc: 0.7480\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8385 - acc: 0.7897 - val_loss: 0.9167 - val_acc: 0.7520\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8381 - acc: 0.7908 - val_loss: 0.9111 - val_acc: 0.7480\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8373 - acc: 0.7919 - val_loss: 0.9347 - val_acc: 0.7370\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8405 - acc: 0.7903 - val_loss: 0.9076 - val_acc: 0.7530\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8384 - acc: 0.7887 - val_loss: 0.9218 - val_acc: 0.7450\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8378 - acc: 0.7905 - val_loss: 0.9169 - val_acc: 0.7490\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8383 - acc: 0.7893 - val_loss: 0.9141 - val_acc: 0.7490\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8365 - acc: 0.7917 - val_loss: 0.9384 - val_acc: 0.7390\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8379 - acc: 0.7895 - val_loss: 0.9174 - val_acc: 0.7470\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8377 - acc: 0.7875 - val_loss: 0.9160 - val_acc: 0.7530\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8369 - acc: 0.7915 - val_loss: 0.9108 - val_acc: 0.7500\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8368 - acc: 0.7919 - val_loss: 0.9219 - val_acc: 0.7520\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8369 - acc: 0.7923 - val_loss: 0.9155 - val_acc: 0.7480\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8375 - acc: 0.7929 - val_loss: 0.9161 - val_acc: 0.7510\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8374 - acc: 0.7893 - val_loss: 0.9077 - val_acc: 0.7530\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8361 - acc: 0.7913 - val_loss: 0.9199 - val_acc: 0.7490\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8344 - acc: 0.7917 - val_loss: 0.9118 - val_acc: 0.7470\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8355 - acc: 0.7909 - val_loss: 0.9076 - val_acc: 0.7530\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8348 - acc: 0.7919 - val_loss: 0.9243 - val_acc: 0.7470\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8341 - acc: 0.7921 - val_loss: 0.9489 - val_acc: 0.7380\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8369 - acc: 0.7951 - val_loss: 0.9210 - val_acc: 0.7460\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8354 - acc: 0.7925 - val_loss: 0.9145 - val_acc: 0.7520\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8340 - acc: 0.7889 - val_loss: 0.9179 - val_acc: 0.7460\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8342 - acc: 0.7931 - val_loss: 0.9086 - val_acc: 0.7540\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8344 - acc: 0.7917 - val_loss: 0.9100 - val_acc: 0.7520\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8338 - acc: 0.7908 - val_loss: 0.9170 - val_acc: 0.7510\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8345 - acc: 0.7904 - val_loss: 0.9082 - val_acc: 0.7470\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8327 - acc: 0.7933 - val_loss: 0.9115 - val_acc: 0.7530\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8329 - acc: 0.7940 - val_loss: 0.9176 - val_acc: 0.7440\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8320 - acc: 0.7944 - val_loss: 0.9327 - val_acc: 0.7460\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8349 - acc: 0.7927 - val_loss: 0.9271 - val_acc: 0.7400\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8330 - acc: 0.7912 - val_loss: 0.9450 - val_acc: 0.7430\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8347 - acc: 0.7891 - val_loss: 0.9328 - val_acc: 0.7430\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8336 - acc: 0.7925 - val_loss: 0.9640 - val_acc: 0.7360\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8349 - acc: 0.7912 - val_loss: 0.9081 - val_acc: 0.7490\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8303 - acc: 0.7920 - val_loss: 0.9242 - val_acc: 0.7480\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8324 - acc: 0.7928 - val_loss: 0.9121 - val_acc: 0.7540\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8339 - acc: 0.7935 - val_loss: 0.9081 - val_acc: 0.7530\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8332 - acc: 0.7909 - val_loss: 0.9167 - val_acc: 0.7470\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8309 - acc: 0.7917 - val_loss: 0.9254 - val_acc: 0.7360\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8315 - acc: 0.7897 - val_loss: 0.9154 - val_acc: 0.7510\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8321 - acc: 0.7913 - val_loss: 0.9287 - val_acc: 0.7510\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8307 - acc: 0.7944 - val_loss: 0.9124 - val_acc: 0.7550\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8312 - acc: 0.7917 - val_loss: 0.9200 - val_acc: 0.7440\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8335 - acc: 0.7929 - val_loss: 0.9190 - val_acc: 0.7500\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8303 - acc: 0.7940 - val_loss: 0.9140 - val_acc: 0.7480\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8309 - acc: 0.7933 - val_loss: 0.9218 - val_acc: 0.7430\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8300 - acc: 0.7912 - val_loss: 0.9105 - val_acc: 0.7500\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8287 - acc: 0.7908 - val_loss: 0.9496 - val_acc: 0.7420\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8300 - acc: 0.7932 - val_loss: 0.9088 - val_acc: 0.7560\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8284 - acc: 0.7935 - val_loss: 0.9101 - val_acc: 0.7500\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8294 - acc: 0.7936 - val_loss: 0.9075 - val_acc: 0.7490\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8279 - acc: 0.7932 - val_loss: 0.9096 - val_acc: 0.7490\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8290 - acc: 0.7901 - val_loss: 0.9680 - val_acc: 0.7210\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8305 - acc: 0.7912 - val_loss: 0.9095 - val_acc: 0.7490\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8271 - acc: 0.7924 - val_loss: 0.9460 - val_acc: 0.7400\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8288 - acc: 0.7937 - val_loss: 0.9146 - val_acc: 0.7460\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8282 - acc: 0.7932 - val_loss: 0.9113 - val_acc: 0.7500\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8289 - acc: 0.7952 - val_loss: 0.9263 - val_acc: 0.7430\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8271 - acc: 0.7931 - val_loss: 0.9104 - val_acc: 0.7550\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8273 - acc: 0.7924 - val_loss: 0.9244 - val_acc: 0.7470\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8273 - acc: 0.7936 - val_loss: 0.9315 - val_acc: 0.7400\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8284 - acc: 0.7915 - val_loss: 0.9530 - val_acc: 0.7290\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8288 - acc: 0.7932 - val_loss: 0.9161 - val_acc: 0.7440\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8278 - acc: 0.7940 - val_loss: 0.9235 - val_acc: 0.7450\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8270 - acc: 0.7939 - val_loss: 0.9058 - val_acc: 0.7530\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8266 - acc: 0.7943 - val_loss: 0.9324 - val_acc: 0.7370\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8287 - acc: 0.7925 - val_loss: 0.9237 - val_acc: 0.7470\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8287 - acc: 0.7957 - val_loss: 0.9061 - val_acc: 0.7470\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8248 - acc: 0.7928 - val_loss: 0.9250 - val_acc: 0.7440\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8258 - acc: 0.7951 - val_loss: 0.9154 - val_acc: 0.7410\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8250 - acc: 0.7959 - val_loss: 0.9342 - val_acc: 0.7410\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8244 - acc: 0.7928 - val_loss: 0.9170 - val_acc: 0.7420\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8235 - acc: 0.7937 - val_loss: 0.9105 - val_acc: 0.7510\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8261 - acc: 0.7935 - val_loss: 0.9076 - val_acc: 0.7550\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8235 - acc: 0.7947 - val_loss: 0.9095 - val_acc: 0.7530\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8239 - acc: 0.7960 - val_loss: 0.9128 - val_acc: 0.7550\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8234 - acc: 0.7952 - val_loss: 0.9197 - val_acc: 0.7410\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8241 - acc: 0.7951 - val_loss: 0.9205 - val_acc: 0.7430\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8262 - acc: 0.7929 - val_loss: 0.9043 - val_acc: 0.7550\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8218 - acc: 0.7961 - val_loss: 0.9105 - val_acc: 0.7530\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8233 - acc: 0.7961 - val_loss: 0.9330 - val_acc: 0.7360\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8236 - acc: 0.7960 - val_loss: 0.9289 - val_acc: 0.7440\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8229 - acc: 0.7941 - val_loss: 0.9116 - val_acc: 0.7530\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8237 - acc: 0.7931 - val_loss: 0.9338 - val_acc: 0.7390\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8235 - acc: 0.7937 - val_loss: 0.9238 - val_acc: 0.7390\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8205 - acc: 0.7965 - val_loss: 0.9138 - val_acc: 0.7490\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8218 - acc: 0.7980 - val_loss: 0.9016 - val_acc: 0.7520\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8224 - acc: 0.7956 - val_loss: 0.9222 - val_acc: 0.7430\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8220 - acc: 0.7936 - val_loss: 0.9208 - val_acc: 0.7460\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8210 - acc: 0.7953 - val_loss: 0.9292 - val_acc: 0.7380\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8222 - acc: 0.7956 - val_loss: 0.9044 - val_acc: 0.7530\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8217 - acc: 0.7967 - val_loss: 0.9147 - val_acc: 0.7490\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8198 - acc: 0.7980 - val_loss: 0.9295 - val_acc: 0.7430\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8205 - acc: 0.7951 - val_loss: 0.9036 - val_acc: 0.7500\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8182 - acc: 0.7965 - val_loss: 0.9040 - val_acc: 0.7490\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8178 - acc: 0.7971 - val_loss: 0.9042 - val_acc: 0.7540\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8198 - acc: 0.7969 - val_loss: 0.9076 - val_acc: 0.7430\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8201 - acc: 0.7963 - val_loss: 0.9243 - val_acc: 0.7400\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8214 - acc: 0.7952 - val_loss: 0.9202 - val_acc: 0.7500\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8186 - acc: 0.7971 - val_loss: 0.9122 - val_acc: 0.7480\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8196 - acc: 0.7951 - val_loss: 0.9079 - val_acc: 0.7640\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8199 - acc: 0.7971 - val_loss: 0.9067 - val_acc: 0.7510\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8175 - acc: 0.7957 - val_loss: 0.9067 - val_acc: 0.7510\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8186 - acc: 0.7956 - val_loss: 0.9013 - val_acc: 0.7530\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8162 - acc: 0.7964 - val_loss: 0.9209 - val_acc: 0.7500\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8197 - acc: 0.7945 - val_loss: 0.9253 - val_acc: 0.7460\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8179 - acc: 0.7975 - val_loss: 0.9059 - val_acc: 0.7540\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8173 - acc: 0.7992 - val_loss: 0.9047 - val_acc: 0.7570\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8157 - acc: 0.7969 - val_loss: 0.9241 - val_acc: 0.7420\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8191 - acc: 0.7989 - val_loss: 0.9056 - val_acc: 0.7500\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8157 - acc: 0.7979 - val_loss: 0.9202 - val_acc: 0.7520\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8153 - acc: 0.7975 - val_loss: 0.9088 - val_acc: 0.7490\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8159 - acc: 0.7985 - val_loss: 0.9083 - val_acc: 0.7480\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8161 - acc: 0.7963 - val_loss: 0.9015 - val_acc: 0.7480\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8161 - acc: 0.7969 - val_loss: 0.9035 - val_acc: 0.7510\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8160 - acc: 0.7981 - val_loss: 0.9122 - val_acc: 0.7480\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8159 - acc: 0.7965 - val_loss: 0.9095 - val_acc: 0.7520\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8153 - acc: 0.7995 - val_loss: 0.9215 - val_acc: 0.7380\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8154 - acc: 0.7968 - val_loss: 0.9275 - val_acc: 0.7480\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8139 - acc: 0.7973 - val_loss: 0.9056 - val_acc: 0.7530\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8136 - acc: 0.7949 - val_loss: 0.9113 - val_acc: 0.7450\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8134 - acc: 0.7996 - val_loss: 0.9037 - val_acc: 0.7580\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8137 - acc: 0.7999 - val_loss: 0.9074 - val_acc: 0.7580\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8155 - acc: 0.7963 - val_loss: 0.9214 - val_acc: 0.7400\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8156 - acc: 0.7984 - val_loss: 0.9021 - val_acc: 0.7440\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8143 - acc: 0.7981 - val_loss: 0.9046 - val_acc: 0.7460\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8131 - acc: 0.7991 - val_loss: 0.9023 - val_acc: 0.7510\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8139 - acc: 0.7984 - val_loss: 0.9365 - val_acc: 0.7390\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8153 - acc: 0.7989 - val_loss: 0.9080 - val_acc: 0.7470\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8108 - acc: 0.7989 - val_loss: 0.9110 - val_acc: 0.7480\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8125 - acc: 0.7969 - val_loss: 0.8996 - val_acc: 0.7580\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8104 - acc: 0.8004 - val_loss: 0.9415 - val_acc: 0.7390\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8130 - acc: 0.7995 - val_loss: 0.9244 - val_acc: 0.7510\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8137 - acc: 0.8000 - val_loss: 0.9010 - val_acc: 0.7560\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8136 - acc: 0.7985 - val_loss: 0.9162 - val_acc: 0.7390\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8098 - acc: 0.8009 - val_loss: 0.9149 - val_acc: 0.7380\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8107 - acc: 0.8012 - val_loss: 0.9146 - val_acc: 0.7470\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8119 - acc: 0.7985 - val_loss: 0.9049 - val_acc: 0.7510\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8122 - acc: 0.8017 - val_loss: 0.8989 - val_acc: 0.7580\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8113 - acc: 0.7971 - val_loss: 0.9198 - val_acc: 0.7420\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8119 - acc: 0.7976 - val_loss: 0.8995 - val_acc: 0.7580\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8127 - acc: 0.7963 - val_loss: 0.9102 - val_acc: 0.7540\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8108 - acc: 0.7997 - val_loss: 0.9026 - val_acc: 0.7520\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8091 - acc: 0.7980 - val_loss: 0.9127 - val_acc: 0.7550\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8123 - acc: 0.7961 - val_loss: 0.9149 - val_acc: 0.7470\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8097 - acc: 0.8015 - val_loss: 0.9120 - val_acc: 0.7550\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8107 - acc: 0.8003 - val_loss: 0.9058 - val_acc: 0.7480\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8104 - acc: 0.8001 - val_loss: 0.9090 - val_acc: 0.7470\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8104 - acc: 0.7992 - val_loss: 0.9076 - val_acc: 0.7490\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8113 - acc: 0.7964 - val_loss: 0.9012 - val_acc: 0.7530\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8097 - acc: 0.7980 - val_loss: 0.9194 - val_acc: 0.7550\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8106 - acc: 0.7985 - val_loss: 0.8977 - val_acc: 0.7570\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8103 - acc: 0.8000 - val_loss: 0.9391 - val_acc: 0.7460\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8080 - acc: 0.7987 - val_loss: 0.9148 - val_acc: 0.7420\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8076 - acc: 0.8007 - val_loss: 0.8958 - val_acc: 0.7550\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8066 - acc: 0.7996 - val_loss: 0.9431 - val_acc: 0.7380\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8072 - acc: 0.7981 - val_loss: 0.8994 - val_acc: 0.7540\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8066 - acc: 0.7988 - val_loss: 0.9086 - val_acc: 0.7470\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8087 - acc: 0.7989 - val_loss: 0.9055 - val_acc: 0.7480\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8090 - acc: 0.8009 - val_loss: 0.9068 - val_acc: 0.7500\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8097 - acc: 0.8020 - val_loss: 0.9014 - val_acc: 0.7540\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8057 - acc: 0.8019 - val_loss: 0.9102 - val_acc: 0.7500\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8079 - acc: 0.8011 - val_loss: 0.9186 - val_acc: 0.7570\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8059 - acc: 0.8008 - val_loss: 0.9161 - val_acc: 0.7420\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8073 - acc: 0.7996 - val_loss: 0.9114 - val_acc: 0.7440\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8065 - acc: 0.7987 - val_loss: 0.8962 - val_acc: 0.7520\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8070 - acc: 0.7992 - val_loss: 0.8997 - val_acc: 0.7570\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8077 - acc: 0.7995 - val_loss: 0.9084 - val_acc: 0.7560\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8067 - acc: 0.7999 - val_loss: 0.9056 - val_acc: 0.7420\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8054 - acc: 0.8013 - val_loss: 0.8968 - val_acc: 0.7550\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8030 - acc: 0.8033 - val_loss: 0.9122 - val_acc: 0.7500\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8062 - acc: 0.8032 - val_loss: 0.9331 - val_acc: 0.7300\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8065 - acc: 0.8035 - val_loss: 0.9178 - val_acc: 0.7410\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8027 - acc: 0.8037 - val_loss: 0.9037 - val_acc: 0.7520\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8051 - acc: 0.8027 - val_loss: 0.9103 - val_acc: 0.7450\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8068 - acc: 0.7980 - val_loss: 0.8982 - val_acc: 0.7550\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8043 - acc: 0.8007 - val_loss: 0.8954 - val_acc: 0.7490\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8023 - acc: 0.8041 - val_loss: 0.9194 - val_acc: 0.7410\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8055 - acc: 0.8037 - val_loss: 0.9061 - val_acc: 0.7470\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8053 - acc: 0.8051 - val_loss: 0.8980 - val_acc: 0.7440\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8047 - acc: 0.8027 - val_loss: 0.9001 - val_acc: 0.7580\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8027 - acc: 0.8020 - val_loss: 0.9153 - val_acc: 0.7440\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8035 - acc: 0.8025 - val_loss: 0.8962 - val_acc: 0.7530\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8029 - acc: 0.8011 - val_loss: 0.8995 - val_acc: 0.7460\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8047 - acc: 0.8016 - val_loss: 0.9178 - val_acc: 0.7430\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8011 - acc: 0.8020 - val_loss: 0.8991 - val_acc: 0.7490\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8017 - acc: 0.8041 - val_loss: 0.9091 - val_acc: 0.7430\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8026 - acc: 0.8009 - val_loss: 0.9061 - val_acc: 0.7570\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8006 - acc: 0.8045 - val_loss: 0.9031 - val_acc: 0.7480\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8015 - acc: 0.8037 - val_loss: 0.9431 - val_acc: 0.7330\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8018 - acc: 0.8045 - val_loss: 0.9056 - val_acc: 0.7530\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8017 - acc: 0.8048 - val_loss: 0.9199 - val_acc: 0.7500\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8008 - acc: 0.8031 - val_loss: 0.9127 - val_acc: 0.7500\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8013 - acc: 0.8013 - val_loss: 0.9033 - val_acc: 0.7510\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8013 - acc: 0.8013 - val_loss: 0.9054 - val_acc: 0.7570\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7990 - acc: 0.8019 - val_loss: 0.9025 - val_acc: 0.7430\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7995 - acc: 0.8051 - val_loss: 0.8959 - val_acc: 0.7480\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7996 - acc: 0.8025 - val_loss: 0.8969 - val_acc: 0.7480\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7985 - acc: 0.8032 - val_loss: 0.9071 - val_acc: 0.7420\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7985 - acc: 0.8033 - val_loss: 0.9048 - val_acc: 0.7550\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7987 - acc: 0.8068 - val_loss: 0.9026 - val_acc: 0.7480\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8002 - acc: 0.8032 - val_loss: 0.9016 - val_acc: 0.7540\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7980 - acc: 0.8044 - val_loss: 0.9485 - val_acc: 0.7280\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7990 - acc: 0.8048 - val_loss: 0.9510 - val_acc: 0.7420\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8020 - acc: 0.8000 - val_loss: 0.9392 - val_acc: 0.7370\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7998 - acc: 0.7995 - val_loss: 0.9199 - val_acc: 0.7490\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7995 - acc: 0.8028 - val_loss: 0.9290 - val_acc: 0.7470\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7999 - acc: 0.8065 - val_loss: 0.9056 - val_acc: 0.7490\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7978 - acc: 0.8039 - val_loss: 0.9174 - val_acc: 0.7440\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7970 - acc: 0.8027 - val_loss: 0.9009 - val_acc: 0.7520\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7971 - acc: 0.8072 - val_loss: 0.8971 - val_acc: 0.7570\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7980 - acc: 0.8043 - val_loss: 0.9015 - val_acc: 0.7480\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7968 - acc: 0.8068 - val_loss: 0.8971 - val_acc: 0.7500\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7980 - acc: 0.8044 - val_loss: 0.9024 - val_acc: 0.7490\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7990 - acc: 0.8016 - val_loss: 0.9154 - val_acc: 0.7450\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7976 - acc: 0.8037 - val_loss: 0.9107 - val_acc: 0.7460\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7975 - acc: 0.8035 - val_loss: 0.9099 - val_acc: 0.7500\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7973 - acc: 0.8040 - val_loss: 0.9141 - val_acc: 0.7510\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7977 - acc: 0.8080 - val_loss: 0.9256 - val_acc: 0.7480\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7985 - acc: 0.8032 - val_loss: 0.9322 - val_acc: 0.7350\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7988 - acc: 0.8051 - val_loss: 0.9032 - val_acc: 0.7470\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7956 - acc: 0.8069 - val_loss: 0.9032 - val_acc: 0.7520\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7959 - acc: 0.8033 - val_loss: 0.8981 - val_acc: 0.7450\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7961 - acc: 0.8069 - val_loss: 1.0276 - val_acc: 0.7110\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8008 - acc: 0.8028 - val_loss: 0.8979 - val_acc: 0.7510\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7957 - acc: 0.8047 - val_loss: 0.9019 - val_acc: 0.7490\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7935 - acc: 0.8069 - val_loss: 0.9218 - val_acc: 0.7420\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7969 - acc: 0.8045 - val_loss: 0.9009 - val_acc: 0.7460\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7940 - acc: 0.8051 - val_loss: 0.8953 - val_acc: 0.7580\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7950 - acc: 0.8052 - val_loss: 0.9094 - val_acc: 0.7500\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7935 - acc: 0.8065 - val_loss: 0.9083 - val_acc: 0.7490\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7927 - acc: 0.8020 - val_loss: 0.9028 - val_acc: 0.7500\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7933 - acc: 0.8053 - val_loss: 0.9188 - val_acc: 0.7450\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7940 - acc: 0.8065 - val_loss: 0.9023 - val_acc: 0.7420\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7953 - acc: 0.8049 - val_loss: 0.8976 - val_acc: 0.7560\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7932 - acc: 0.8072 - val_loss: 0.8955 - val_acc: 0.7600\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7930 - acc: 0.8087 - val_loss: 0.9166 - val_acc: 0.7480\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7952 - acc: 0.8051 - val_loss: 0.8945 - val_acc: 0.7540\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7927 - acc: 0.8072 - val_loss: 0.9035 - val_acc: 0.7560\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7948 - acc: 0.8057 - val_loss: 0.9159 - val_acc: 0.7380\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7924 - acc: 0.8084 - val_loss: 0.9315 - val_acc: 0.7410\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7942 - acc: 0.8052 - val_loss: 0.9107 - val_acc: 0.7490\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7931 - acc: 0.8104 - val_loss: 0.8999 - val_acc: 0.7520\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7894 - acc: 0.8097 - val_loss: 0.9112 - val_acc: 0.7540\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7927 - acc: 0.8072 - val_loss: 0.9010 - val_acc: 0.7550\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7916 - acc: 0.8081 - val_loss: 0.9086 - val_acc: 0.7520\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7923 - acc: 0.8047 - val_loss: 0.9114 - val_acc: 0.7520\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7891 - acc: 0.8096 - val_loss: 0.9101 - val_acc: 0.7490\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7915 - acc: 0.8069 - val_loss: 0.9431 - val_acc: 0.7260\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7930 - acc: 0.8080 - val_loss: 0.9472 - val_acc: 0.7440\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7916 - acc: 0.8051 - val_loss: 0.9323 - val_acc: 0.7380\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7924 - acc: 0.8071 - val_loss: 0.8944 - val_acc: 0.7560\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7907 - acc: 0.8057 - val_loss: 0.8926 - val_acc: 0.7470\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7914 - acc: 0.8095 - val_loss: 0.9014 - val_acc: 0.7530\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7910 - acc: 0.8071 - val_loss: 0.9000 - val_acc: 0.7450\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7879 - acc: 0.8124 - val_loss: 0.9080 - val_acc: 0.7440\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7894 - acc: 0.8085 - val_loss: 0.9452 - val_acc: 0.7420\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7915 - acc: 0.8101 - val_loss: 0.9044 - val_acc: 0.7460\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7889 - acc: 0.8064 - val_loss: 0.9064 - val_acc: 0.7530\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7911 - acc: 0.8077 - val_loss: 0.9808 - val_acc: 0.7230\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7926 - acc: 0.8076 - val_loss: 0.9033 - val_acc: 0.7510\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7901 - acc: 0.8088 - val_loss: 0.9005 - val_acc: 0.7480\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7881 - acc: 0.8095 - val_loss: 0.9167 - val_acc: 0.7510\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7932 - acc: 0.8045 - val_loss: 0.9428 - val_acc: 0.7320\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7883 - acc: 0.8051 - val_loss: 0.8981 - val_acc: 0.7540\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7886 - acc: 0.8080 - val_loss: 0.9082 - val_acc: 0.7540\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7871 - acc: 0.8080 - val_loss: 0.9845 - val_acc: 0.7170\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7880 - acc: 0.8117 - val_loss: 0.9103 - val_acc: 0.7610\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7875 - acc: 0.8092 - val_loss: 0.8979 - val_acc: 0.7480\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7876 - acc: 0.8136 - val_loss: 0.8999 - val_acc: 0.7560\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7864 - acc: 0.8105 - val_loss: 0.9187 - val_acc: 0.7540\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7891 - acc: 0.8087 - val_loss: 0.9035 - val_acc: 0.7460\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7891 - acc: 0.8084 - val_loss: 0.9002 - val_acc: 0.7500\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7865 - acc: 0.8108 - val_loss: 0.8920 - val_acc: 0.7560\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7865 - acc: 0.8113 - val_loss: 0.8958 - val_acc: 0.7550\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7848 - acc: 0.8100 - val_loss: 0.8995 - val_acc: 0.7510\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7867 - acc: 0.8107 - val_loss: 0.9058 - val_acc: 0.7520\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7835 - acc: 0.8128 - val_loss: 0.9142 - val_acc: 0.7600\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7891 - acc: 0.8095 - val_loss: 0.8967 - val_acc: 0.7480\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7849 - acc: 0.8099 - val_loss: 0.9194 - val_acc: 0.7440\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7853 - acc: 0.8117 - val_loss: 0.9117 - val_acc: 0.7490\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7855 - acc: 0.8100 - val_loss: 0.9079 - val_acc: 0.7510\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7851 - acc: 0.8108 - val_loss: 0.8964 - val_acc: 0.7550\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7861 - acc: 0.8112 - val_loss: 0.9126 - val_acc: 0.7520\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7867 - acc: 0.8096 - val_loss: 0.9083 - val_acc: 0.7440\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7856 - acc: 0.8103 - val_loss: 0.8981 - val_acc: 0.7560\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7856 - acc: 0.8087 - val_loss: 0.9553 - val_acc: 0.7210\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7837 - acc: 0.8121 - val_loss: 0.9018 - val_acc: 0.7520\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7862 - acc: 0.8119 - val_loss: 0.9077 - val_acc: 0.7560\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7840 - acc: 0.8124 - val_loss: 0.8967 - val_acc: 0.7540\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 28us/step\n",
      "1500/1500 [==============================] - 0s 29us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7768123442173004, 0.8202666666666667]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9055443687438964, 0.7713333333333333]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.9679 - acc: 0.1485 - val_loss: 1.9435 - val_acc: 0.1610\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.9617 - acc: 0.1473 - val_loss: 1.9358 - val_acc: 0.1910\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9448 - acc: 0.1657 - val_loss: 1.9300 - val_acc: 0.2010\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9380 - acc: 0.1715 - val_loss: 1.9240 - val_acc: 0.2010\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9343 - acc: 0.1751 - val_loss: 1.9183 - val_acc: 0.2120\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9318 - acc: 0.1792 - val_loss: 1.9123 - val_acc: 0.2160\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9144 - acc: 0.1909 - val_loss: 1.9057 - val_acc: 0.2080\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9093 - acc: 0.1977 - val_loss: 1.8978 - val_acc: 0.2110\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9037 - acc: 0.1985 - val_loss: 1.8894 - val_acc: 0.2090\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8973 - acc: 0.2123 - val_loss: 1.8804 - val_acc: 0.2140\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8892 - acc: 0.2092 - val_loss: 1.8718 - val_acc: 0.2210\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8830 - acc: 0.2193 - val_loss: 1.8620 - val_acc: 0.2320\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8791 - acc: 0.2140 - val_loss: 1.8519 - val_acc: 0.2310\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8655 - acc: 0.2288 - val_loss: 1.8405 - val_acc: 0.2380\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8570 - acc: 0.2345 - val_loss: 1.8294 - val_acc: 0.2590\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8441 - acc: 0.2419 - val_loss: 1.8169 - val_acc: 0.2690\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8360 - acc: 0.2463 - val_loss: 1.8040 - val_acc: 0.2830\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8264 - acc: 0.2600 - val_loss: 1.7900 - val_acc: 0.3000\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8161 - acc: 0.2629 - val_loss: 1.7755 - val_acc: 0.3090\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8073 - acc: 0.2671 - val_loss: 1.7617 - val_acc: 0.3260\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7895 - acc: 0.2812 - val_loss: 1.7452 - val_acc: 0.3470\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7773 - acc: 0.2901 - val_loss: 1.7290 - val_acc: 0.3630\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7697 - acc: 0.2972 - val_loss: 1.7132 - val_acc: 0.3790\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7558 - acc: 0.3056 - val_loss: 1.6969 - val_acc: 0.3960\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7340 - acc: 0.3117 - val_loss: 1.6774 - val_acc: 0.4090\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7291 - acc: 0.3223 - val_loss: 1.6587 - val_acc: 0.4300\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7160 - acc: 0.3277 - val_loss: 1.6413 - val_acc: 0.4580\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7007 - acc: 0.3341 - val_loss: 1.6224 - val_acc: 0.4730\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6864 - acc: 0.3392 - val_loss: 1.6031 - val_acc: 0.4750\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6674 - acc: 0.3539 - val_loss: 1.5839 - val_acc: 0.4870\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6493 - acc: 0.3588 - val_loss: 1.5643 - val_acc: 0.5110\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6408 - acc: 0.3671 - val_loss: 1.5439 - val_acc: 0.5210\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6300 - acc: 0.3761 - val_loss: 1.5252 - val_acc: 0.5320\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5948 - acc: 0.3939 - val_loss: 1.5018 - val_acc: 0.5400\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5877 - acc: 0.3941 - val_loss: 1.4809 - val_acc: 0.5470\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5734 - acc: 0.3969 - val_loss: 1.4614 - val_acc: 0.5640\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5538 - acc: 0.4092 - val_loss: 1.4416 - val_acc: 0.5670\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5416 - acc: 0.4131 - val_loss: 1.4206 - val_acc: 0.5690\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5274 - acc: 0.4237 - val_loss: 1.4017 - val_acc: 0.5750\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5096 - acc: 0.4327 - val_loss: 1.3796 - val_acc: 0.5850\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4925 - acc: 0.4433 - val_loss: 1.3582 - val_acc: 0.5930\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4736 - acc: 0.4485 - val_loss: 1.3361 - val_acc: 0.5960\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4671 - acc: 0.4473 - val_loss: 1.3180 - val_acc: 0.5980\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4493 - acc: 0.4545 - val_loss: 1.2992 - val_acc: 0.6010\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4227 - acc: 0.4605 - val_loss: 1.2780 - val_acc: 0.6080\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4234 - acc: 0.4680 - val_loss: 1.2625 - val_acc: 0.6170\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3967 - acc: 0.4824 - val_loss: 1.2408 - val_acc: 0.6240\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3900 - acc: 0.4796 - val_loss: 1.2228 - val_acc: 0.6300\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3859 - acc: 0.4836 - val_loss: 1.2058 - val_acc: 0.6370\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3589 - acc: 0.4979 - val_loss: 1.1889 - val_acc: 0.6360\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3430 - acc: 0.5043 - val_loss: 1.1716 - val_acc: 0.6400\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3344 - acc: 0.5069 - val_loss: 1.1590 - val_acc: 0.6470\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3220 - acc: 0.5007 - val_loss: 1.1459 - val_acc: 0.6470\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3065 - acc: 0.5179 - val_loss: 1.1280 - val_acc: 0.6460\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2941 - acc: 0.5217 - val_loss: 1.1105 - val_acc: 0.6570\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2835 - acc: 0.5212 - val_loss: 1.0965 - val_acc: 0.6630\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2722 - acc: 0.5303 - val_loss: 1.0849 - val_acc: 0.6670\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2701 - acc: 0.5237 - val_loss: 1.0719 - val_acc: 0.6700\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2569 - acc: 0.5359 - val_loss: 1.0587 - val_acc: 0.6680\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2333 - acc: 0.5479 - val_loss: 1.0431 - val_acc: 0.6680\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2330 - acc: 0.5411 - val_loss: 1.0340 - val_acc: 0.6750\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2167 - acc: 0.5468 - val_loss: 1.0240 - val_acc: 0.6730\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2162 - acc: 0.5496 - val_loss: 1.0104 - val_acc: 0.6780\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1972 - acc: 0.5563 - val_loss: 1.0018 - val_acc: 0.6800\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1853 - acc: 0.5597 - val_loss: 0.9908 - val_acc: 0.6780\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1699 - acc: 0.5639 - val_loss: 0.9774 - val_acc: 0.6830\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1843 - acc: 0.5611 - val_loss: 0.9686 - val_acc: 0.6890\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1529 - acc: 0.5696 - val_loss: 0.9602 - val_acc: 0.6860\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1559 - acc: 0.5740 - val_loss: 0.9493 - val_acc: 0.6930\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1513 - acc: 0.5700 - val_loss: 0.9428 - val_acc: 0.6890\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1389 - acc: 0.5761 - val_loss: 0.9345 - val_acc: 0.6930\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1250 - acc: 0.5823 - val_loss: 0.9254 - val_acc: 0.6980\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1180 - acc: 0.5889 - val_loss: 0.9176 - val_acc: 0.7020\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1179 - acc: 0.5877 - val_loss: 0.9095 - val_acc: 0.7090\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1097 - acc: 0.5839 - val_loss: 0.9011 - val_acc: 0.7070\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1133 - acc: 0.5791 - val_loss: 0.8961 - val_acc: 0.7100\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0998 - acc: 0.5877 - val_loss: 0.8874 - val_acc: 0.7100\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0781 - acc: 0.6051 - val_loss: 0.8798 - val_acc: 0.7110\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.0722 - acc: 0.601 - 0s 27us/step - loss: 1.0714 - acc: 0.6011 - val_loss: 0.8731 - val_acc: 0.7100\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0634 - acc: 0.6051 - val_loss: 0.8669 - val_acc: 0.7120\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0707 - acc: 0.6069 - val_loss: 0.8599 - val_acc: 0.7150\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0600 - acc: 0.6049 - val_loss: 0.8558 - val_acc: 0.7190\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0589 - acc: 0.6072 - val_loss: 0.8506 - val_acc: 0.7190\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0517 - acc: 0.6144 - val_loss: 0.8425 - val_acc: 0.7170\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0282 - acc: 0.6224 - val_loss: 0.8364 - val_acc: 0.7200\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0323 - acc: 0.6183 - val_loss: 0.8322 - val_acc: 0.7200\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0288 - acc: 0.6187 - val_loss: 0.8248 - val_acc: 0.7210\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0160 - acc: 0.6256 - val_loss: 0.8194 - val_acc: 0.7240\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0239 - acc: 0.6231 - val_loss: 0.8143 - val_acc: 0.7220\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0066 - acc: 0.6280 - val_loss: 0.8119 - val_acc: 0.7250\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0173 - acc: 0.6259 - val_loss: 0.8073 - val_acc: 0.7220\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0085 - acc: 0.6261 - val_loss: 0.8045 - val_acc: 0.7250\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9879 - acc: 0.6383 - val_loss: 0.8002 - val_acc: 0.7230\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9987 - acc: 0.6368 - val_loss: 0.7962 - val_acc: 0.7250\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9868 - acc: 0.6323 - val_loss: 0.7912 - val_acc: 0.7240\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9985 - acc: 0.6287 - val_loss: 0.7891 - val_acc: 0.7250\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9806 - acc: 0.6353 - val_loss: 0.7854 - val_acc: 0.7280\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9827 - acc: 0.6329 - val_loss: 0.7823 - val_acc: 0.7250\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9745 - acc: 0.6433 - val_loss: 0.7777 - val_acc: 0.7280\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9716 - acc: 0.6369 - val_loss: 0.7744 - val_acc: 0.7330\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9648 - acc: 0.6452 - val_loss: 0.7716 - val_acc: 0.7360\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9619 - acc: 0.6452 - val_loss: 0.7678 - val_acc: 0.7290\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9641 - acc: 0.6389 - val_loss: 0.7655 - val_acc: 0.7300\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9548 - acc: 0.6487 - val_loss: 0.7647 - val_acc: 0.7270\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9317 - acc: 0.6571 - val_loss: 0.7566 - val_acc: 0.7350\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9546 - acc: 0.6455 - val_loss: 0.7560 - val_acc: 0.7340\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9507 - acc: 0.6497 - val_loss: 0.7540 - val_acc: 0.7320\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9285 - acc: 0.6620 - val_loss: 0.7497 - val_acc: 0.7290\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9473 - acc: 0.6536 - val_loss: 0.7472 - val_acc: 0.7300\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9208 - acc: 0.6596 - val_loss: 0.7462 - val_acc: 0.7270\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9289 - acc: 0.6647 - val_loss: 0.7415 - val_acc: 0.7330\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9219 - acc: 0.6569 - val_loss: 0.7402 - val_acc: 0.7300\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9132 - acc: 0.6619 - val_loss: 0.7373 - val_acc: 0.7320\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9234 - acc: 0.6609 - val_loss: 0.7362 - val_acc: 0.7330\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9254 - acc: 0.6537 - val_loss: 0.7346 - val_acc: 0.7360\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9082 - acc: 0.6681 - val_loss: 0.7322 - val_acc: 0.7290\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9105 - acc: 0.6640 - val_loss: 0.7287 - val_acc: 0.7350\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9031 - acc: 0.6760 - val_loss: 0.7245 - val_acc: 0.7390\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9120 - acc: 0.6657 - val_loss: 0.7253 - val_acc: 0.7330\n",
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8879 - acc: 0.6720 - val_loss: 0.7217 - val_acc: 0.7360\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8974 - acc: 0.6636 - val_loss: 0.7200 - val_acc: 0.7360\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8991 - acc: 0.6687 - val_loss: 0.7167 - val_acc: 0.7410\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8757 - acc: 0.6793 - val_loss: 0.7148 - val_acc: 0.7390\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8944 - acc: 0.6617 - val_loss: 0.7132 - val_acc: 0.7400\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8877 - acc: 0.6736 - val_loss: 0.7140 - val_acc: 0.7330\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8777 - acc: 0.6773 - val_loss: 0.7104 - val_acc: 0.7380\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8735 - acc: 0.6817 - val_loss: 0.7062 - val_acc: 0.7440\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8706 - acc: 0.6820 - val_loss: 0.7043 - val_acc: 0.7450\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8579 - acc: 0.6895 - val_loss: 0.7027 - val_acc: 0.7430\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8769 - acc: 0.6715 - val_loss: 0.6991 - val_acc: 0.7460\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8503 - acc: 0.6915 - val_loss: 0.6983 - val_acc: 0.7470\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8491 - acc: 0.6883 - val_loss: 0.6957 - val_acc: 0.7450\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8612 - acc: 0.6831 - val_loss: 0.6965 - val_acc: 0.7460\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8667 - acc: 0.6856 - val_loss: 0.6962 - val_acc: 0.7470\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8676 - acc: 0.6747 - val_loss: 0.6924 - val_acc: 0.7470\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8385 - acc: 0.6856 - val_loss: 0.6929 - val_acc: 0.7490\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8447 - acc: 0.6913 - val_loss: 0.6914 - val_acc: 0.7490\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8441 - acc: 0.6912 - val_loss: 0.6887 - val_acc: 0.7460\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8429 - acc: 0.6883 - val_loss: 0.6867 - val_acc: 0.7480\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8474 - acc: 0.6873 - val_loss: 0.6882 - val_acc: 0.7490\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8325 - acc: 0.6872 - val_loss: 0.6864 - val_acc: 0.7470\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8327 - acc: 0.6915 - val_loss: 0.6850 - val_acc: 0.7470\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8275 - acc: 0.6867 - val_loss: 0.6835 - val_acc: 0.7520\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8338 - acc: 0.6931 - val_loss: 0.6846 - val_acc: 0.7440\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8245 - acc: 0.7007 - val_loss: 0.6776 - val_acc: 0.7480\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8315 - acc: 0.6913 - val_loss: 0.6796 - val_acc: 0.7520\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8361 - acc: 0.6948 - val_loss: 0.6774 - val_acc: 0.7510\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8194 - acc: 0.6961 - val_loss: 0.6795 - val_acc: 0.7450\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8168 - acc: 0.6931 - val_loss: 0.6762 - val_acc: 0.7490\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8223 - acc: 0.7009 - val_loss: 0.6769 - val_acc: 0.7480\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8184 - acc: 0.6932 - val_loss: 0.6748 - val_acc: 0.7450\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8116 - acc: 0.7007 - val_loss: 0.6727 - val_acc: 0.7510\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8097 - acc: 0.7012 - val_loss: 0.6699 - val_acc: 0.7460\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7991 - acc: 0.7072 - val_loss: 0.6700 - val_acc: 0.7450\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8113 - acc: 0.6973 - val_loss: 0.6671 - val_acc: 0.7510\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8030 - acc: 0.7033 - val_loss: 0.6658 - val_acc: 0.7500\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8093 - acc: 0.7044 - val_loss: 0.6647 - val_acc: 0.7470\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7959 - acc: 0.7029 - val_loss: 0.6651 - val_acc: 0.7480\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8077 - acc: 0.7085 - val_loss: 0.6657 - val_acc: 0.7450\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8081 - acc: 0.6935 - val_loss: 0.6623 - val_acc: 0.7490\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7932 - acc: 0.7055 - val_loss: 0.6638 - val_acc: 0.7470\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7981 - acc: 0.7056 - val_loss: 0.6628 - val_acc: 0.7460\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7911 - acc: 0.7081 - val_loss: 0.6598 - val_acc: 0.7500\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7852 - acc: 0.7011 - val_loss: 0.6575 - val_acc: 0.7530\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7927 - acc: 0.7013 - val_loss: 0.6588 - val_acc: 0.7470\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7903 - acc: 0.7029 - val_loss: 0.6578 - val_acc: 0.7500\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7955 - acc: 0.7060 - val_loss: 0.6549 - val_acc: 0.7530\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7857 - acc: 0.7120 - val_loss: 0.6571 - val_acc: 0.7470\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7859 - acc: 0.7047 - val_loss: 0.6546 - val_acc: 0.7500\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7837 - acc: 0.7105 - val_loss: 0.6538 - val_acc: 0.7510\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7741 - acc: 0.7151 - val_loss: 0.6529 - val_acc: 0.7520\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7771 - acc: 0.7121 - val_loss: 0.6542 - val_acc: 0.7520\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7622 - acc: 0.7143 - val_loss: 0.6520 - val_acc: 0.7480\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7708 - acc: 0.7116 - val_loss: 0.6494 - val_acc: 0.7510\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7743 - acc: 0.7039 - val_loss: 0.6500 - val_acc: 0.7550\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7752 - acc: 0.7159 - val_loss: 0.6481 - val_acc: 0.7570\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7683 - acc: 0.7171 - val_loss: 0.6477 - val_acc: 0.7530\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7597 - acc: 0.7167 - val_loss: 0.6485 - val_acc: 0.7520\n",
      "Epoch 179/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7636 - acc: 0.7141 - val_loss: 0.6489 - val_acc: 0.7500\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7585 - acc: 0.7179 - val_loss: 0.6468 - val_acc: 0.7550\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7650 - acc: 0.7109 - val_loss: 0.6453 - val_acc: 0.7530\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7597 - acc: 0.7144 - val_loss: 0.6477 - val_acc: 0.7530\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7718 - acc: 0.7087 - val_loss: 0.6465 - val_acc: 0.7540\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7476 - acc: 0.7171 - val_loss: 0.6449 - val_acc: 0.7520\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7584 - acc: 0.7148 - val_loss: 0.6433 - val_acc: 0.7520\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7407 - acc: 0.7233 - val_loss: 0.6444 - val_acc: 0.7540\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7607 - acc: 0.7175 - val_loss: 0.6428 - val_acc: 0.7560\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7550 - acc: 0.7193 - val_loss: 0.6431 - val_acc: 0.7530\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7371 - acc: 0.7255 - val_loss: 0.6410 - val_acc: 0.7540\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7421 - acc: 0.7208 - val_loss: 0.6416 - val_acc: 0.7550\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7436 - acc: 0.7233 - val_loss: 0.6400 - val_acc: 0.7570\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7540 - acc: 0.7215 - val_loss: 0.6430 - val_acc: 0.7580\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7399 - acc: 0.7217 - val_loss: 0.6400 - val_acc: 0.7570\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7379 - acc: 0.7267 - val_loss: 0.6397 - val_acc: 0.7580\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7407 - acc: 0.7203 - val_loss: 0.6382 - val_acc: 0.7560\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7394 - acc: 0.7273 - val_loss: 0.6387 - val_acc: 0.7620\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7318 - acc: 0.7219 - val_loss: 0.6382 - val_acc: 0.7580\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7419 - acc: 0.7259 - val_loss: 0.6366 - val_acc: 0.7610\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7347 - acc: 0.7229 - val_loss: 0.6375 - val_acc: 0.7590\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7399 - acc: 0.7217 - val_loss: 0.6378 - val_acc: 0.7610\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 31us/step\n",
      "1500/1500 [==============================] - 0s 32us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45404278625647226, 0.8336]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6286560386021932, 0.7619999995231629]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 1.8892 - acc: 0.2248 - val_loss: 1.8392 - val_acc: 0.2683\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.7618 - acc: 0.3386 - val_loss: 1.6829 - val_acc: 0.3820\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.5665 - acc: 0.4663 - val_loss: 1.4649 - val_acc: 0.5173\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.3395 - acc: 0.5798 - val_loss: 1.2459 - val_acc: 0.6223\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.1392 - acc: 0.6458 - val_loss: 1.0725 - val_acc: 0.6660\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.9911 - acc: 0.6841 - val_loss: 0.9475 - val_acc: 0.6977\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.8869 - acc: 0.7046 - val_loss: 0.8614 - val_acc: 0.7110\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.8150 - acc: 0.7210 - val_loss: 0.8040 - val_acc: 0.7280\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.7640 - acc: 0.7332 - val_loss: 0.7591 - val_acc: 0.7373\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.7256 - acc: 0.7425 - val_loss: 0.7286 - val_acc: 0.7467\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.6958 - acc: 0.7517 - val_loss: 0.7005 - val_acc: 0.7540\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6714 - acc: 0.7592 - val_loss: 0.6796 - val_acc: 0.7600\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6513 - acc: 0.7651 - val_loss: 0.6632 - val_acc: 0.7613\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.6338 - acc: 0.7708 - val_loss: 0.6506 - val_acc: 0.7620\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.6188 - acc: 0.7762 - val_loss: 0.6358 - val_acc: 0.7710\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6050 - acc: 0.7812 - val_loss: 0.6264 - val_acc: 0.7713\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5933 - acc: 0.7851 - val_loss: 0.6162 - val_acc: 0.7770\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5822 - acc: 0.7898 - val_loss: 0.6096 - val_acc: 0.7790\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5721 - acc: 0.7940 - val_loss: 0.6010 - val_acc: 0.7817\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.5629 - acc: 0.7983 - val_loss: 0.5939 - val_acc: 0.7833\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5542 - acc: 0.8015 - val_loss: 0.5880 - val_acc: 0.7873\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5460 - acc: 0.8039 - val_loss: 0.5830 - val_acc: 0.7867\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5388 - acc: 0.8076 - val_loss: 0.5776 - val_acc: 0.7873\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5312 - acc: 0.8107 - val_loss: 0.5731 - val_acc: 0.7893\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5246 - acc: 0.8129 - val_loss: 0.5717 - val_acc: 0.7880\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5185 - acc: 0.8161 - val_loss: 0.5664 - val_acc: 0.7917\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5123 - acc: 0.8182 - val_loss: 0.5638 - val_acc: 0.7923\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5064 - acc: 0.8193 - val_loss: 0.5618 - val_acc: 0.7907\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5011 - acc: 0.8225 - val_loss: 0.5586 - val_acc: 0.7957\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4956 - acc: 0.8240 - val_loss: 0.5545 - val_acc: 0.7953\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4907 - acc: 0.8267 - val_loss: 0.5511 - val_acc: 0.7987\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4857 - acc: 0.8287 - val_loss: 0.5509 - val_acc: 0.7970\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4813 - acc: 0.8299 - val_loss: 0.5492 - val_acc: 0.8010\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4767 - acc: 0.8321 - val_loss: 0.5462 - val_acc: 0.7947\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4723 - acc: 0.8331 - val_loss: 0.5456 - val_acc: 0.8003\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4678 - acc: 0.8337 - val_loss: 0.5428 - val_acc: 0.7993\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4638 - acc: 0.8372 - val_loss: 0.5414 - val_acc: 0.7987\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4600 - acc: 0.8384 - val_loss: 0.5412 - val_acc: 0.8020\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4561 - acc: 0.8400 - val_loss: 0.5403 - val_acc: 0.7977\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4526 - acc: 0.8412 - val_loss: 0.5382 - val_acc: 0.8013\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4488 - acc: 0.8425 - val_loss: 0.5399 - val_acc: 0.8007\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4454 - acc: 0.8437 - val_loss: 0.5356 - val_acc: 0.8010\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4425 - acc: 0.8451 - val_loss: 0.5339 - val_acc: 0.8023\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4390 - acc: 0.8454 - val_loss: 0.5361 - val_acc: 0.8030\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4357 - acc: 0.8467 - val_loss: 0.5358 - val_acc: 0.8047\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4328 - acc: 0.8492 - val_loss: 0.5332 - val_acc: 0.8020\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4299 - acc: 0.8506 - val_loss: 0.5326 - val_acc: 0.8010\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4269 - acc: 0.8506 - val_loss: 0.5325 - val_acc: 0.8040\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4239 - acc: 0.8521 - val_loss: 0.5343 - val_acc: 0.8047\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4208 - acc: 0.8530 - val_loss: 0.5326 - val_acc: 0.8017\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4187 - acc: 0.8548 - val_loss: 0.5304 - val_acc: 0.8033\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4155 - acc: 0.8548 - val_loss: 0.5320 - val_acc: 0.8060\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4132 - acc: 0.8550 - val_loss: 0.5316 - val_acc: 0.8060\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4107 - acc: 0.8560 - val_loss: 0.5288 - val_acc: 0.8053\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.4079 - acc: 0.8582 - val_loss: 0.5318 - val_acc: 0.8060\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4059 - acc: 0.8582 - val_loss: 0.5316 - val_acc: 0.8053\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4032 - acc: 0.8601 - val_loss: 0.5311 - val_acc: 0.8057\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4009 - acc: 0.8594 - val_loss: 0.5288 - val_acc: 0.8053\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3988 - acc: 0.8612 - val_loss: 0.5300 - val_acc: 0.8067\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3963 - acc: 0.8622 - val_loss: 0.5313 - val_acc: 0.8057\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3940 - acc: 0.8615 - val_loss: 0.5295 - val_acc: 0.8060\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3917 - acc: 0.8638 - val_loss: 0.5303 - val_acc: 0.8063\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3901 - acc: 0.8657 - val_loss: 0.5304 - val_acc: 0.8070\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3878 - acc: 0.8654 - val_loss: 0.5319 - val_acc: 0.8070\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3857 - acc: 0.8665 - val_loss: 0.5319 - val_acc: 0.8073\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3837 - acc: 0.8662 - val_loss: 0.5306 - val_acc: 0.8083\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3817 - acc: 0.8679 - val_loss: 0.5319 - val_acc: 0.8083\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3796 - acc: 0.8685 - val_loss: 0.5311 - val_acc: 0.8087\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3781 - acc: 0.8687 - val_loss: 0.5314 - val_acc: 0.8090\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3762 - acc: 0.8687 - val_loss: 0.5331 - val_acc: 0.8080\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3739 - acc: 0.8709 - val_loss: 0.5362 - val_acc: 0.8083\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3722 - acc: 0.8710 - val_loss: 0.5317 - val_acc: 0.8093\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3708 - acc: 0.8719 - val_loss: 0.5325 - val_acc: 0.8053\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3689 - acc: 0.8723 - val_loss: 0.5363 - val_acc: 0.8057\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3672 - acc: 0.8730 - val_loss: 0.5351 - val_acc: 0.8070\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3652 - acc: 0.8747 - val_loss: 0.5338 - val_acc: 0.8077\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3636 - acc: 0.8742 - val_loss: 0.5357 - val_acc: 0.8080\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3619 - acc: 0.8745 - val_loss: 0.5359 - val_acc: 0.8070\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3601 - acc: 0.8758 - val_loss: 0.5374 - val_acc: 0.8060\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3581 - acc: 0.8762 - val_loss: 0.5347 - val_acc: 0.8083\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3566 - acc: 0.8768 - val_loss: 0.5362 - val_acc: 0.8060\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3553 - acc: 0.8775 - val_loss: 0.5374 - val_acc: 0.8070\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.3538 - acc: 0.8779 - val_loss: 0.5392 - val_acc: 0.8063\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.3519 - acc: 0.8787 - val_loss: 0.5379 - val_acc: 0.8077\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3505 - acc: 0.8794 - val_loss: 0.5389 - val_acc: 0.8027\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3487 - acc: 0.8787 - val_loss: 0.5404 - val_acc: 0.8080\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3478 - acc: 0.8802 - val_loss: 0.5393 - val_acc: 0.8083\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3459 - acc: 0.8806 - val_loss: 0.5411 - val_acc: 0.8090\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3443 - acc: 0.8800 - val_loss: 0.5425 - val_acc: 0.8070\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3430 - acc: 0.8811 - val_loss: 0.5412 - val_acc: 0.8063\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3416 - acc: 0.8819 - val_loss: 0.5417 - val_acc: 0.8087\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3404 - acc: 0.8824 - val_loss: 0.5425 - val_acc: 0.8063\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3389 - acc: 0.8834 - val_loss: 0.5445 - val_acc: 0.8070\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3374 - acc: 0.8841 - val_loss: 0.5463 - val_acc: 0.8053\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.3360 - acc: 0.8837 - val_loss: 0.5454 - val_acc: 0.8047\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3345 - acc: 0.8852 - val_loss: 0.5481 - val_acc: 0.8067\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.3335 - acc: 0.8845 - val_loss: 0.5465 - val_acc: 0.8070\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3319 - acc: 0.8856 - val_loss: 0.5485 - val_acc: 0.8060\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3303 - acc: 0.8852 - val_loss: 0.5514 - val_acc: 0.8053\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3293 - acc: 0.8865 - val_loss: 0.5527 - val_acc: 0.8053\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3283 - acc: 0.8869 - val_loss: 0.5512 - val_acc: 0.8033\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3264 - acc: 0.8877 - val_loss: 0.5523 - val_acc: 0.8033\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3254 - acc: 0.8880 - val_loss: 0.5528 - val_acc: 0.8053\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3244 - acc: 0.8884 - val_loss: 0.5534 - val_acc: 0.8077\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3230 - acc: 0.8879 - val_loss: 0.5533 - val_acc: 0.8037\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3219 - acc: 0.8885 - val_loss: 0.5551 - val_acc: 0.8050\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3206 - acc: 0.8892 - val_loss: 0.5555 - val_acc: 0.8070\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3195 - acc: 0.8899 - val_loss: 0.5566 - val_acc: 0.8060\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3180 - acc: 0.8894 - val_loss: 0.5563 - val_acc: 0.8060\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3169 - acc: 0.8906 - val_loss: 0.5592 - val_acc: 0.8067\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3156 - acc: 0.8917 - val_loss: 0.5589 - val_acc: 0.8040\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3147 - acc: 0.8918 - val_loss: 0.5605 - val_acc: 0.8043\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3134 - acc: 0.8919 - val_loss: 0.5587 - val_acc: 0.8070\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3123 - acc: 0.8916 - val_loss: 0.5627 - val_acc: 0.8040\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3106 - acc: 0.8939 - val_loss: 0.5620 - val_acc: 0.8043\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3105 - acc: 0.8927 - val_loss: 0.5624 - val_acc: 0.8063\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3086 - acc: 0.8933 - val_loss: 0.5643 - val_acc: 0.8027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3079 - acc: 0.8936 - val_loss: 0.5667 - val_acc: 0.8013\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3064 - acc: 0.8945 - val_loss: 0.5651 - val_acc: 0.8043\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3053 - acc: 0.8942 - val_loss: 0.5679 - val_acc: 0.8013\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 44us/step\n",
      "4000/4000 [==============================] - 0s 44us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.30109437415094087, 0.8965151515151515]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5874011704921722, 0.80425]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
